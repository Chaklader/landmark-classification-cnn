# C-6: Object Detection and Segmentation

<br>
<br>

1. **Computer Vision Task Hierarchy**
    - Image classification vs. localization vs. detection vs. segmentation
2. **Object Localization**
    - Multi-head architecture
    - Loss functions
    - Bounding box representations
3. **Object Detection Fundamentals**
    - One-stage vs. two-stage detection
    - Anchor-based approaches
    - RetinaNet architecture
    - Feature Pyramid Networks
    - Focal Loss
4. **Object Detection Evaluation**
    - Precision and Recall
    - Intersection over Union (IoU)
    - Mean Average Precision (mAP)
    - Mean Average Recall (mAR)
5. **Semantic Segmentation**
    - UNet architecture
    - Skip connections
    - Dice Loss

#### Computer Vision Task Hierarchy

Computer vision encompasses a spectrum of increasingly complex tasks, each building upon the capabilities of simpler
ones while adding new dimensions of spatial understanding. This hierarchy of tasks forms a progression from basic
image-level understanding to detailed pixel-wise comprehension, with each level requiring more sophisticated
architectural approaches and presenting unique challenges.

##### Image Classification

At the foundation of computer vision lies image classification—the task of assigning one or more labels to an entire
image based on its visual content. This represents the most basic form of image understanding.

###### Key Characteristics

In image classification, the model answers a fundamental question: "What is in this image?" The output is a set of class
probabilities or scores indicating the likelihood that the image contains instances of different categories. For
example, an image might be classified as containing a "dog" with 95% confidence and a "cat" with 2% confidence.

The architectural approach typically involves:

- A convolutional neural network backbone that extracts hierarchical features
- A global pooling layer that aggregates spatial information
- One or more fully connected layers that map these features to class scores

###### Limitations

While powerful, image classification has significant limitations:

- It provides no spatial information about object locations
- It cannot differentiate between multiple instances of the same class
- It struggles with images containing multiple distinct objects

These limitations motivate the progression to more spatially-aware vision tasks.

##### Object Localization

Object localization extends classification by not only identifying what is in an image but also where the primary object
is located. It bridges the gap between image-level and object-level understanding.

###### Key Characteristics

In object localization, the model answers both "what" and "where" questions but focuses solely on the most prominent
object in the image. The output includes:

- A class label (or class probabilities)
- A bounding box defined by four coordinates that enclose the object

This task usually employs a multi-head architecture where:

- One head handles classification (identical to image classification)
- A parallel head predicts the bounding box coordinates

The loss function combines classification loss (typically cross-entropy) with a regression loss for bounding box
prediction (such as mean squared error or smooth L1 loss).

###### Limitations

While more spatially aware than classification, object localization still has constraints:

- It typically handles only a single dominant object per image
- It cannot address scenes with multiple objects of interest
- It provides only a rectangular approximation of the object's location, not its exact shape

##### Object Detection

Object detection represents a significant leap in complexity by identifying and localizing all objects of interest
within an image, regardless of their number or class.

###### Key Characteristics

In object detection, the model must simultaneously:

- Identify all instances of objects from known classes
- Predict accurate bounding boxes for each instance
- Handle variable numbers of objects across different images

The output consists of a list of detected objects, each with:

- A class label
- A confidence score
- A bounding box

Object detection architectures generally fall into two categories:

- Two-stage detectors like R-CNN variants that first propose regions of interest, then classify them
- One-stage detectors like YOLO and RetinaNet that predict classes and bounding boxes in a single forward pass

Both approaches must address the challenge of predicting objects at different scales and aspect ratios, typically
through techniques like:

- Anchor boxes (predefined boxes of different shapes)
- Feature pyramid networks (multi-scale feature representations)
- Non-maximum suppression (removing redundant detections)

###### Limitations

While powerful, object detection still has constraints:

- It provides only rectangular bounding boxes, not precise object shapes
- It cannot distinguish between touching or overlapping instances of the same class
- It operates at the object level rather than the pixel level

##### Semantic Segmentation

Semantic segmentation elevates vision understanding to the pixel level by classifying every pixel in an image according
to the object or region it belongs to.

###### Key Characteristics

In semantic segmentation, the model creates a dense, pixel-wise classification map where:

- Each pixel is assigned to exactly one class
- The output has the same spatial dimensions as the input image
- The prediction preserves the exact shape and boundaries of objects

The typical architecture follows an encoder-decoder structure:

- The encoder progressively reduces spatial dimensions while increasing feature depth
- The decoder recovers spatial information while reducing feature depth
- Skip connections often link corresponding encoder and decoder levels to preserve fine details

Common architectures include U-Net, FCN (Fully Convolutional Networks), and DeepLab variants.

###### Limitations

While offering pixel-perfect classification, semantic segmentation cannot distinguish between separate instances of the
same class—all "person" pixels are labeled identically, regardless of whether they belong to one person or many.

##### Instance Segmentation

At the apex of the computer vision task hierarchy, instance segmentation combines the instance-awareness of object
detection with the pixel-precision of semantic segmentation.

###### Key Characteristics

Instance segmentation requires the model to:

- Identify each individual object instance
- Classify each instance according to its class
- Precisely delineate the pixels belonging to each instance

This allows separate treatment of distinct objects from the same class—identifying not just "three people" but "person
1, person 2, and person 3," each with their exact pixel mask.

Popular architectures include:

- Mask R-CNN, which extends Faster R-CNN with a parallel branch for mask prediction
- YOLACT, which combines one-stage detection with prototype masks

Instance segmentation represents the most complete form of scene understanding among these tasks, offering both what and
where information at the finest possible granularity.

##### Progression of Complexity

The hierarchy of computer vision tasks represents a progression along multiple dimensions:

1. **Spatial granularity**: From image-level (classification) to object-level (detection) to pixel-level (segmentation)
2. **Output complexity**: From simple class labels to multiple coordinates to dense pixel maps
3. **Instance awareness**: From class-only to individual object instances
4. **Computational demands**: Increasing computational requirements at each level

Understanding this hierarchy helps in selecting the appropriate technique for a given application based on the required
level of detail and available computational resources. It also illustrates how advances in computer vision have
progressively enabled machines to perceive images with increasingly human-like understanding—from merely recognizing
content to precisely locating and delineating objects in complex scenes.

#### Object Localization

Object localization extends basic image classification by not only identifying what appears in an image but also
precisely where it is located. This spatial awareness represents a fundamental step toward more comprehensive scene
understanding, bridging the gap between simple classification and the more complex tasks of detection and segmentation.

##### Multi-Head Architecture

Object localization employs a distinctive multi-head neural network design that elegantly handles the dual tasks of
classification and spatial localization.

###### Shared Backbone

The foundation of an object localization network is a shared feature extraction backbone, typically a convolutional
neural network such as ResNet, EfficientNet, or VGG. This backbone serves a critical role:

1. It processes the raw input image through multiple convolutional layers
2. It progressively extracts increasingly abstract visual features
3. It creates rich feature representations that contain both semantic and spatial information

The backbone's ability to preserve some degree of spatial information while extracting meaningful features makes it
ideal for the dual requirements of localization and classification.

###### Classification Head

Branching from the shared backbone, the classification head focuses exclusively on determining the object's category:

1. It typically consists of one or more fully-connected layers
2. It often includes global pooling to aggregate spatial information
3. It terminates in a layer with neurons corresponding to possible classes
4. It produces a vector of class scores or probabilities

This head functions almost identically to the final layers in a standard classification network but shares its initial
feature extraction with the localization task.

###### Localization Head

In parallel with the classification head, the localization head specializes in predicting the object's spatial extent:

1. It consists of one or more fully-connected layers
2. It maintains access to the spatial information in the backbone's features
3. It terminates in a layer with typically four neurons
4. It outputs coordinates defining the object's bounding box

The localization head learns to map the spatial patterns in the feature maps to precise coordinate predictions,
essentially translating abstract representations back into image-space coordinates.

###### Forward Pass Flow

During inference, the object localization network processes an image through a streamlined flow:

1. The input image passes through the shared backbone, generating feature maps
2. These feature maps simultaneously feed into both the classification and localization heads
3. The classification head produces class probabilities
4. The localization head generates bounding box coordinates
5. The network returns both outputs as a combined result

This parallel processing allows efficient computation of both "what" and "where" information in a single forward pass.

##### Loss Functions

Training an object localization network requires careful consideration of how to quantify errors in both classification
and bounding box prediction.

###### Combined Loss Formulation

The overall loss function for object localization typically takes the form of a weighted sum:

$$L = L_{cls} + \alpha \cdot L_{loc}$$

Where:

- $L_{cls}$ represents the classification loss
- $L_{loc}$ represents the localization loss
- $\alpha$ is a weighting hyperparameter that balances the two components

This formulation allows joint optimization of both tasks while controlling their relative importance.

###### Classification Loss

For the classification component, cross-entropy loss is the standard choice:

$$L_{cls} = -\log(\hat{p}_y)$$

Where $\hat{p}_y$ is the predicted probability for the true class $y$.

This loss penalizes the network when it assigns low probability to the correct class, encouraging confident and accurate
classification.

###### Localization Loss

For bounding box regression, mean squared error (MSE) is commonly used:

$$L_{loc} = \frac{1}{4}\sum_{i=1}^{4}(b_i - \hat{b}_i)^2$$

Where:

- $b_i$ represents the four ground truth bounding box coordinates
- $\hat{b}_i$ represents the corresponding predicted coordinates

This loss measures the squared difference between predicted and actual coordinates, penalizing spatial inaccuracy.

###### Loss Balancing

The hyperparameter $\alpha$ plays a crucial role in training dynamics:

1. If $\alpha$ is too small, the network might excel at classification but produce poor bounding boxes
2. If $\alpha$ is too large, bounding box prediction might improve at the expense of classification accuracy
3. The optimal value depends on the relative scales of the losses and the dataset characteristics

Finding the right balance often requires experimentation, though values between 0.5 and 5 serve as common starting
points.

###### Advanced Loss Variants

Beyond basic MSE, several specialized losses have been developed for bounding box regression:

1. **Smooth L1 Loss**: Combines the stability of L2 loss for small errors with the robustness of L1 loss for large
   errors
2. **IoU Loss**: Directly optimizes the Intersection over Union metric
3. **GIoU/DIoU/CIoU Losses**: Refinements that address limitations of the basic IoU loss

These advanced losses often provide better convergence and more accurate localization.

##### Bounding Box Representations

The way bounding boxes are represented significantly impacts network training and prediction quality.

###### Coordinate Formats

Two primary formats exist for representing bounding boxes:

1. **Corner format**: Specifies the top-left and bottom-right corners $$[x_{min}, y_{min}, x_{max}, y_{max}]$$

    This format directly describes the box boundaries but can be sensitive to scale.

2. **Center format**: Specifies the center point, width, and height $$[x_{center}, y_{center}, width, height]$$

    This format often provides more stable gradients during training.

Each format has advantages in different contexts, with center format generally preferred for regression tasks.

###### Coordinate Normalization

Raw pixel coordinates can cause training instability due to their potentially large range. To address this, coordinates
are typically normalized:

1. **Image-relative normalization**: All coordinates are divided by image dimensions, resulting in values between 0 and
   1 $$[x/W, y/H, w/W, h/H]$$
2. **Feature map normalization**: Coordinates are expressed relative to the feature map from which they're predicted

Normalization ensures that coordinate values stay within a reasonable range regardless of image size, improving training
stability.

###### Anchor-Based Prediction

Advanced localization systems often predict bounding boxes as offsets from predefined reference boxes called anchors:

1. Instead of direct coordinates, the network predicts transformation parameters
2. These parameters are applied to canonical anchor boxes to produce final predictions
3. This approach simplifies the regression problem, especially for multiple objects

For a center format box with anchor $(x_a, y_a, w_a, h_a)$, the network might predict offsets $(t_x, t_y, t_w, t_h)$
that are transformed:

$$x = x_a + t_x \cdot w_a$$ $$y = y_a + t_y \cdot h_a$$ $$w = w_a \cdot e^{t_w}$$ $$h = h_a \cdot e^{t_h}$$

This parameterization ensures that predictions stay reasonably close to realistic object proportions.

###### Regression Targets

During training, ground truth boxes must be converted to regression targets compatible with the network's output format:

1. For direct coordinate prediction, this might involve simple normalization
2. For anchor-based approaches, targets are the transform parameters that would convert anchors to ground truth boxes

Proper target generation is crucial for effective training, as it translates the human-annotated boxes into the
mathematical space in which the network operates.

Object localization serves as both a valuable capability in its own right and a foundational component of more complex
vision tasks like object detection. By understanding the multi-head architecture, loss function design, and bounding box
representation, we gain insight into how deep learning bridges the gap between recognizing objects and understanding
their spatial presence in the visual world.

#### Object Detection Fundamentals

Object detection extends beyond simple localization by identifying and localizing multiple objects of potentially
different classes within a single image. This capability forms the foundation for numerous applications from autonomous
driving to medical imaging. Understanding the core principles and architectures of object detection reveals how deep
learning models can effectively process complex visual scenes with multiple subjects of interest.

##### One-Stage vs. Two-Stage Detection

The object detection landscape is primarily divided into two architectural paradigms, each with distinct approaches to
the detection process.

###### Two-Stage Detection Framework

Two-stage detectors decompose the detection problem into sequential steps:

1. **Region Proposal**: The first stage generates candidate regions that might contain objects
    - Traditional methods used selective search or edge box algorithms
    - Modern approaches like Region Proposal Networks (RPNs) learn to propose regions
    - Typically generates 1,000-2,000 region proposals per image
2. **Classification and Refinement**: The second stage processes each proposal
    - Extracts features from each region using ROI pooling or similar techniques
    - Classifies the region contents (including a "background" class)
    - Refines the bounding box coordinates for better localization
    - May add additional tasks like mask prediction (in Mask R-CNN)

This family includes influential architectures like R-CNN, Fast R-CNN, Faster R-CNN, and Mask R-CNN, each representing
evolutionary improvements to the two-stage paradigm.

###### One-Stage Detection Framework

One-stage detectors tackle classification and localization simultaneously in a single forward pass:

1. They divide the image into a grid or use predefined anchor points
2. For each grid cell or anchor, they predict:
    - Class probabilities (including background)
    - Bounding box coordinates or offsets
    - Confidence scores indicating object presence
3. They process the entire image in a single network pass without intermediate region selection

Popular one-stage detectors include YOLO (You Only Look Once), SSD (Single Shot MultiBox Detector), and RetinaNet.

###### Comparative Analysis

Each approach offers distinct advantages and limitations:

**Two-Stage Strengths**:

- Generally higher detection accuracy, especially for small objects
- More flexible region processing
- Better handling of object overlaps

**Two-Stage Limitations**:

- Slower inference speed due to sequential processing
- More complex architecture and training process
- Higher computational requirements

**One-Stage Strengths**:

- Faster inference speeds, often suitable for real-time applications
- Simpler architectural design
- End-to-end training without intermediate steps

**One-Stage Limitations**:

- Historically lower accuracy (though gap has narrowed with advances like RetinaNet)
- Class imbalance challenges with background vs. foreground examples
- Fixed grid or anchor design may not optimally fit all object shapes

The choice between paradigms often involves balancing speed requirements against accuracy needs for specific
applications.

##### Anchor-Based Approaches

Anchors serve as reference boxes that enable detectors to handle objects of varying scales and aspect ratios
efficiently.

###### Anchor Fundamentals

Anchors are predefined bounding boxes with specific:

- Positions across the image
- Scales (sizes)
- Aspect ratios (height-to-width proportions)

They function as spatial hypotheses from which the network predicts adjustments, rather than predicting box coordinates
from scratch.

###### Anchor Generation Process

The process for generating an anchor set typically involves:

1. **Grid Definition**: The input image or feature maps are divided into a grid
2. **Anchor Placement**: Anchors are centered at each grid cell or regularly spaced positions
3. **Scale Variation**: Multiple scales are used at each position to capture objects of different sizes
4. **Aspect Ratio Variation**: Different height-width ratios accommodate various object shapes

A typical configuration might include 3 scales and 3 aspect ratios, resulting in 9 anchors per grid position.

###### Prediction Mechanism

For each anchor, the network predicts:

1. **Classification scores**: Probabilities for each object class plus background
2. **Bounding box refinements**: Offsets that transform the anchor into a precise object boundary

The transformation from anchor to predicted box typically follows:

$$x = x_a + w_a \cdot t_x$$ $$y = y_a + h_a \cdot t_y$$ $$w = w_a \cdot e^{t_w}$$ $$h = h_a \cdot e^{t_h}$$

Where $(x_a, y_a, w_a, h_a)$ are anchor parameters and $(t_x, t_y, t_w, t_h)$ are predicted transformations.

###### Training Considerations

During training, anchors are assigned to ground truth objects based on Intersection over Union (IoU):

- Anchors with high IoU to a ground truth box become positive examples
- Anchors with low IoU to all ground truth boxes become negative (background) examples
- Anchors in between are typically ignored during training

This assignment process creates the training targets for both classification and regression.

###### Anchor Design Impact

The design of the anchor set significantly influences detector performance:

- Too few anchors may miss objects with unusual shapes
- Too many anchors increase computational cost and can destabilize training
- Appropriate anchor distribution across scales is critical for detecting both large and small objects

Modern approaches often use dataset analysis to determine optimal anchor configurations.

##### RetinaNet Architecture

RetinaNet represents a breakthrough one-stage detector that combines architectural innovations with a novel loss
function to achieve state-of-the-art performance.

###### Architectural Overview

RetinaNet follows a clear architectural structure:

1. **Backbone**: Typically ResNet with Feature Pyramid Network integration
2. **Classification Subnet**: Predicts class probabilities for each anchor
3. **Box Regression Subnet**: Predicts bounding box refinements for each anchor

This design maintains the speed advantages of one-stage detectors while addressing their historical accuracy
limitations.

###### Classification Subnet

The classification subnet processes each level of the feature pyramid:

1. Four 3×3 convolutional layers with 256 filters, each followed by ReLU
2. A final 3×3 convolutional layer with $K \cdot A$ filters, where:
    - $K$ is the number of classes
    - $A$ is the number of anchors per location

This subnet is applied to each pyramid level with shared weights, producing class predictions for all anchors.

###### Box Regression Subnet

The box regression subnet mirrors the classification subnet's structure:

1. Four 3×3 convolutional layers with 256 filters, each followed by ReLU
2. A final 3×3 convolutional layer with $4 \cdot A$ filters (4 coordinates for each anchor)

Like the classification subnet, its weights are shared across pyramid levels for consistent feature interpretation.

###### Multi-Scale Detection

RetinaNet predicts objects at multiple scales through its feature pyramid integration:

- Larger objects are detected in higher pyramid levels (with larger receptive fields)
- Smaller objects are detected in lower pyramid levels (with finer spatial resolution)
- Anchors of different scales are assigned to appropriate pyramid levels

This multi-scale approach enables effective detection across a wide range of object sizes.

##### Feature Pyramid Networks

Feature Pyramid Networks (FPN) provide a structured approach to multi-scale feature extraction, addressing the challenge
of detecting objects across different scales.

###### Traditional Feature Pyramids

Before FPN, multi-scale detection typically used one of two approaches:

1. **Image pyramid**: Running the detector on multiple rescaled versions of the input image
    - Effective but computationally expensive
    - Requires multiple forward passes
2. **Feature hierarchy**: Using different layers from the CNN backbone
    - Efficient but limited by semantic gaps between shallow and deep features
    - Lower layers had fine resolution but weak semantics

###### FPN Architecture

FPN combines the best aspects of both approaches through a two-pathway structure:

1. **Bottom-up pathway**: The traditional CNN forward pass
    - Progressively reduces spatial dimensions
    - Increases semantic information
    - Creates a hierarchy of feature maps at different scales
2. **Top-down pathway**: Upsampling of higher-level features
    - Starts from the semantically strongest features at the top
    - Progressively upsamples to recover spatial resolution
    - Creates feature maps matching the scales of the bottom-up pathway
3. **Lateral connections**: Connect corresponding levels between pathways
    - Feature maps from the bottom-up pathway are combined with upsampled features
    - Typically implemented as 1×1 convolutions followed by addition
    - Enriches upsampled features with spatial information from earlier layers

###### Feature Map Generation

The combined process generates a set of feature maps ${P_i}$ with the following characteristics:

1. Each level has the same channel dimension (typically 256)
2. Each level has a spatial resolution corresponding to its hierarchy level
3. Each level contains both high-level semantic information and appropriate spatial detail

These properties make the resulting feature maps ideal for detecting objects at their corresponding scales.

###### Implementation Details

A typical FPN implementation includes:

1. Selection of backbone feature maps from different stages (often C2, C3, C4, C5 in ResNet)
2. 1×1 convolutions on these maps to create lateral connections
3. Upsampling of higher-level features, typically using nearest-neighbor interpolation
4. Element-wise addition of lateral and upsampled features
5. 3×3 convolutions to create the final feature maps, reducing aliasing effects

The resulting feature pyramid provides strong multi-scale features with relatively low computational overhead.

##### Focal Loss

Focal Loss addresses a fundamental challenge in one-stage object detection: the extreme class imbalance between
foreground and background examples.

###### Class Imbalance Problem

One-stage detectors face a severe imbalance challenge:

1. They typically evaluate 100,000+ candidate locations per image
2. Only a tiny fraction of these contain objects (often <100)
3. The vast majority are easy-to-classify background examples
4. This imbalance can overwhelm the loss from rare positive examples

Standard cross-entropy loss gives equal weight to all examples, allowing easy negatives to dominate the gradient and
destabilize training.

###### Cross-Entropy Limitations

For a binary classification problem, the standard cross-entropy loss is:

$$\text{CE}(p, y) = -y \log(p) - (1-y) \log(1-p)$$

Where:

- $y \in {0, 1}$ is the ground truth class
- $p \in [0, 1]$ is the model's estimated probability for class 1

For well-classified examples (high $p$ for positive examples, low $p$ for negative examples), this loss still assigns
non-negligible values, allowing easy examples to dominate during training.

###### Focal Loss Formulation

Focal Loss modifies cross-entropy by adding a modulating factor:

$$\text{FL}(p_t) = -(1-p_t)^\gamma \log(p_t)$$

Where:

- $p_t$ is $p$ for positive examples ($y=1$) and $1-p$ for negative examples ($y=0$)
- $\gamma \geq 0$ is the focusing parameter that adjusts the rate at which easy examples are downweighted

This formulation has several key properties:

1. When $\gamma = 0$, Focal Loss is equivalent to standard cross-entropy
2. As $\gamma$ increases, the relative loss for well-classified examples is reduced
3. For hard examples where $p_t$ is small, the loss remains largely unchanged
4. For easy examples where $p_t$ is close to 1, the loss is significantly down-weighted

###### Effect on Training Dynamics

Focal Loss transforms training dynamics in several ways:

1. It effectively focuses training on hard examples that the model struggles with
2. It prevents easy background examples from overwhelming the loss
3. It eliminates the need for hard negative mining or similar sampling strategies
4. It allows the use of a dense set of anchors without sampling or reweighting

###### Implementation Details

In practice, an alpha-balanced version is often used:

$$\text{FL}(p_t) = -\alpha_t (1-p_t)^\gamma \log(p_t)$$

Where $\alpha_t$ is a weighting factor that can further address class imbalance.

Typical values include:

- $\gamma = 2$ for the focusing parameter
- $\alpha = 0.25$ for positive examples, $1-\alpha$ for negative examples

These settings effectively balance the contribution of positive and negative examples while focusing on hard examples.

The combination of anchor-based approaches, Feature Pyramid Networks, and Focal Loss in RetinaNet exemplifies how modern
object detection architectures address the fundamental challenges of localizing and classifying multiple objects across
varying scales, positions, and classes—all while maintaining computational efficiency. These innovations have
collectively transformed object detection from a challenging research problem into a practical technology deployed
across numerous real-world applications.

#### Object Detection Evaluation

Evaluating object detection models requires specialized metrics that assess both localization accuracy and
classification performance. Unlike simpler tasks like image classification where accuracy suffices, object detection
evaluation must account for the spatial component of predictions and handle the complexities of multiple objects per
image. A robust evaluation framework helps researchers and practitioners compare different detection approaches
objectively and identify areas for improvement.

##### Precision and Recall Fundamentals

Precision and recall form the foundational metrics for evaluating object detection performance, adapted from their
origins in information retrieval and binary classification.

###### Defining Detection Outcomes

In object detection, the four possible outcomes for each predicted box are:

1. **True Positive (TP)**: A detection that correctly identifies an object with sufficient overlap with the ground truth
   box
2. **False Positive (FP)**: A detection that either identifies the wrong object class or identifies an object where none
   exists
3. **False Negative (FN)**: A ground truth object that the detector fails to identify
4. **True Negative (TN)**: Correctly not detecting an object where none exists (rarely used in object detection
   evaluation)

Unlike in classification, determining whether a detection is correct requires spatial analysis through Intersection over
Union (IoU) thresholds, as well as class verification.

###### Precision Calculation

Precision measures the accuracy of positive predictions, answering: "Of all the objects the model detected, what
fraction were actually correct?"

$$\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}$$

High precision indicates that when the model predicts an object, it's likely to be correct—the model makes few false
alarms.

###### Recall Calculation

Recall measures the completeness of positive predictions, answering: "Of all the actual objects in the images, what
fraction did the model detect?"

$$\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}$$

High recall indicates that the model finds most of the actual objects—it misses few ground truth instances.

###### The Precision-Recall Tradeoff

Precision and recall typically exhibit an inverse relationship:

1. Increasing detection confidence threshold:
    - Improves precision (fewer false positives)
    - Reduces recall (more missed detections)
2. Decreasing detection confidence threshold:
    - Reduces precision (more false positives)
    - Improves recall (fewer missed detections)

This tradeoff is visualized through precision-recall curves, which plot precision against recall at various confidence
thresholds.

###### Application-Specific Considerations

Different applications prioritize these metrics differently:

1. **Safety-critical systems** (automated driving): May prioritize recall to ensure no objects are missed
2. **User-facing applications** (photo organization): May prioritize precision to avoid annoying false positives
3. **Balanced applications** (surveillance): May seek an optimal balance through F1-score or other combined metrics

Understanding this tradeoff helps practitioners select appropriate operating points for their specific use cases.

##### Intersection over Union (IoU)

Intersection over Union provides the spatial evaluation component unique to object detection, measuring how well
predicted bounding boxes align with ground truth.

###### Definition and Calculation

IoU quantifies the overlap between two bounding boxes as the ratio of their intersection area to their union area:

$$\text{IoU} = \frac{\text{Area of Intersection}}{\text{Area of Union}}$$

For two rectangular boxes $A$ and $B$:

1. Calculate intersection:
   $I = \max(0, \min(A_{x2}, B_{x2}) - \max(A_{x1}, B_{x1})) \times \max(0, \min(A_{y2}, B_{y2}) - \max(A_{y1}, B_{y1}))$
2. Calculate areas: $A_{area} = (A_{x2} - A_{x1}) \times (A_{y2} - A_{y1})$ and
   $B_{area} = (B_{x2} - B_{x1}) \times (B_{y2} - B_{y1})$
3. Calculate union: $U = A_{area} + B_{area} - I$
4. Compute IoU: $\text{IoU} = I / U$

The resulting value ranges from 0 (no overlap) to 1 (perfect overlap).

###### IoU Thresholds

In object detection evaluation, an IoU threshold determines when a detection is considered a match to a ground truth
object:

1. A common threshold is 0.5, meaning boxes must overlap by at least 50% to be considered a match
2. More stringent evaluations use higher thresholds like 0.75 or 0.9
3. Modern evaluation protocols often report metrics at multiple IoU thresholds

The choice of threshold significantly impacts reported performance metrics—higher thresholds demand more precise
localization.

###### Matching Process

In images with multiple objects, determining matches involves:

1. Sorting detections by confidence score
2. Processing detections from highest to lowest confidence
3. Assigning each detection to at most one ground truth based on IoU
4. Once a ground truth is matched, it cannot be matched again

This process ensures that each ground truth object is matched with at most one detection, typically the one with highest
confidence that meets the IoU threshold.

###### IoU Limitations and Extensions

While effective, standard IoU has limitations:

1. It treats all spatial misalignments equally, regardless of direction
2. It doesn't account for the distance between non-overlapping boxes
3. It's sensitive to small errors for tiny objects

These limitations have motivated extensions like:

1. **Generalized IoU (GIoU)**: Accounts for the enclosing box of both prediction and ground truth
2. **Distance IoU (DIoU)**: Incorporates the distance between box centers
3. **Complete IoU (CIoU)**: Adds consideration of aspect ratio similarity

These refined metrics can provide more nuanced evaluation of localization quality.

##### Mean Average Precision (mAP)

Mean Average Precision serves as the primary summary metric for object detection performance, addressing both the
precision-recall tradeoff and multiclass evaluation.

###### Average Precision Calculation

Average Precision (AP) computes the area under the precision-recall curve for a specific class:

1. Generate the precision-recall curve by varying the confidence threshold
2. Apply interpolation to smooth the curve (addressing zigzag patterns)
3. Calculate the area under this interpolated curve

Mathematically, this is approximated as:

$$\text{AP} = \sum_{i} (r_{i+1} - r_i) \times p_{\text{interp}}(r_{i+1})$$

Where $p_{\text{interp}}(r)$ is the interpolated precision at recall level $r$.

###### Interpolation Methods

Different detection benchmarks use varying interpolation approaches:

1. **Pascal VOC (2007)**: Uses 11-point interpolation, sampling recall at [0, 0.1, ..., 1.0]
2. **Pascal VOC (2010-2012)**: Uses all points interpolation
3. **COCO**: Uses 101-point interpolation, sampling recall more densely

Modern implementations typically use all-point interpolation for greater precision.

###### Mean AP Across Classes

The mean Average Precision (mAP) extends AP to multiclass detection scenarios:

$$\text{mAP} = \frac{1}{N} \sum_{c=1}^{N} \text{AP}_c$$

Where:

- $N$ is the number of classes
- $\text{AP}_c$ is the Average Precision for class $c$

This averaging ensures that each class contributes equally to the final metric, regardless of its frequency in the
dataset.

###### IoU Variations in mAP

Modern object detection challenges report mAP across multiple IoU thresholds:

1. **mAP@0.5**: Uses a single IoU threshold of 0.5 (traditional Pascal VOC metric)
2. **mAP@0.75**: Uses a more stringent IoU threshold of 0.75
3. **mAP@[.5:.95]**: Averages mAP over multiple IoU thresholds from 0.5 to 0.95 in steps of 0.05 (COCO primary metric)

The last approach rewards detectors that produce highly accurate bounding boxes while still accounting for reasonable
localization.

###### Interpreting mAP

When analyzing mAP values:

1. Higher values indicate better overall detection performance
2. Gaps between mAP@0.5 and mAP@0.75 reveal localization precision
3. Class-specific AP highlights per-class performance variations
4. Compare different models using the same mAP definition, as variations in calculation can significantly affect
   reported numbers

A comprehensive evaluation often reports both overall mAP and class-specific AP to provide a complete performance
picture.

##### Mean Average Recall (mAR)

While less commonly used as a primary metric, Mean Average Recall provides valuable complementary information about a
detector's ability to find objects.

###### Average Recall Calculation

Average Recall (AR) measures the recall averaged over a range of IoU thresholds:

$$\text{AR} = \frac{2}{0.5} \int_{0.5}^{1.0} \text{Recall(IoU)} , d\text{IoU}$$

In practice, this integral is approximated by sampling at discrete IoU thresholds.

The factor of 2 normalizes the result to [0,1], since the integration range is [0.5,1.0] rather than [0,1].

###### AR Variations

AR is often computed with various constraints:

1. **AR@k**: Maximum of k detections per image (e.g., AR@1, AR@10, AR@100)
2. **AR-small/medium/large**: AR for objects of different size categories

These variations help evaluate recall performance under different operational constraints or for objects of different
scales.

###### Mean AR Across Classes

Similar to mAP, mean Average Recall (mAR) averages AR across all classes:

$$\text{mAR} = \frac{1}{N} \sum_{c=1}^{N} \text{AR}_c$$

Where:

- $N$ is the number of classes
- $\text{AR}_c$ is the Average Recall for class $c$

###### AR as a Complementary Metric

mAR serves several valuable roles in comprehensive evaluation:

1. It indicates a detector's theoretical maximum performance if its confidence ranking were perfect
2. It helps identify whether performance limitations stem from poor localization or poor confidence estimation
3. It's particularly valuable for applications where finding all instances is critical, regardless of confidence

The combination of mAP and mAR provides a more complete picture than either metric alone.

##### Practical Evaluation Considerations

Beyond the core metrics, several practical considerations affect meaningful object detection evaluation.

###### Benchmark Standards

Major object detection benchmarks have established standard evaluation protocols:

1. **PASCAL VOC**: Pioneered mAP@0.5 evaluation for 20 common object classes
2. **COCO**: Expanded to 80 classes with more sophisticated metrics including mAP@[.5:.95]
3. **Open Images**: Features hierarchical class relationships and instance segmentation evaluation

Following these established protocols enables fair comparison with published methods.

###### Scale-Specific Evaluation

Object scale significantly impacts detection difficulty, leading to scale-stratified evaluation:

1. **Small objects**: Area < 32² pixels
2. **Medium objects**: 32² to 96² pixels
3. **Large objects**: Area > 96² pixels

Reporting performance by scale category helps identify specific strengths and weaknesses of different detection
approaches.

###### Other Important Factors

Comprehensive evaluation should consider:

1. **Inference speed**: Often measured in frames per second (FPS) or milliseconds per image
2. **Model size**: Parameter count and memory requirements
3. **Hardware requirements**: GPU memory needs and computational complexity
4. **Edge cases**: Performance under occlusion, unusual viewpoints, or poor lighting

These practical considerations often determine a detector's suitability for real-world deployment beyond benchmark
performance.

###### Error Analysis

Breaking down detection errors provides valuable insights:

1. **Classification errors**: Correct localization but wrong class
2. **Localization errors**: Correct class but insufficient IoU
3. **Duplicate detection errors**: Multiple detections of the same object
4. **Background errors**: False positives in background regions
5. **Missed detection errors**: False negatives

Identifying the dominant error types helps guide focused improvement efforts.

Thorough evaluation using precision, recall, IoU, mAP, and mAR provides a comprehensive assessment of object detection
performance. These metrics collectively capture a detector's ability to accurately locate objects, classify them
correctly, and maintain high confidence in its predictions. By understanding these evaluation approaches, practitioners
can both select appropriate models for their applications and systematically improve detection performance.

#### Semantic Segmentation

Semantic segmentation represents the pinnacle of dense prediction tasks in computer vision, assigning a class label to
every pixel in an image. Unlike object detection which surrounds objects with bounding boxes, semantic segmentation
provides precise object boundaries and can identify amorphous regions like sky, road, or vegetation. This pixel-perfect
understanding enables applications from autonomous driving to medical image analysis, where exact boundaries matter
critically.

##### UNet Architecture

The UNet architecture has emerged as one of the most influential and widely-adopted frameworks for semantic
segmentation, particularly in medical imaging but increasingly across diverse domains.

###### Architectural Overview

UNet's distinctive U-shaped architecture consists of three major components:

1. **Contracting Path (Encoder)**: A series of convolutional blocks followed by downsampling operations that
   progressively reduce spatial dimensions while increasing feature depth.
2. **Bottleneck**: The lowest resolution section connecting the contracting and expansive paths, containing the most
   abstract representations.
3. **Expansive Path (Decoder)**: A series of upsampling operations followed by convolutional blocks that progressively
   increase spatial dimensions while decreasing feature depth.

This symmetric design creates a U-shaped profile when visualized, giving the architecture its name.

###### Contracting Path Details

The encoder follows a typical CNN pattern:

1. Each level consists of two or more 3×3 convolutional layers followed by non-linear activations (typically ReLU)
2. Feature maps double in number at each downsampling step (e.g., 64→128→256→512)
3. Downsampling occurs via 2×2 max pooling operations with stride 2, halving the spatial dimensions
4. The progressive reduction in resolution creates a hierarchical feature representation

This contracting process extracts increasingly abstract features while reducing spatial dimensions, effectively
compressing the image information.

###### Expansive Path Details

The decoder mirrors the encoder but works in reverse:

1. Each level begins with an upsampling operation (transposed convolution or interpolation followed by convolution)
2. Upsampled features are combined with corresponding encoder features via skip connections
3. After feature combination, two or more 3×3 convolutional layers with non-linear activations refine the features
4. Feature maps halve in number at each upsampling step (e.g., 512→256→128→64)

This expansive process gradually recovers spatial resolution while incorporating both high-level semantic information
and low-level spatial details.

###### Final Classification Layer

The network concludes with a 1×1 convolutional layer that maps the feature channels to the number of classes:

1. For binary segmentation, a single output channel with sigmoid activation
2. For multi-class segmentation, C output channels (where C is the number of classes) with softmax activation

This final layer makes the per-pixel classification decision based on the rich feature representation built through the
network.

##### Skip Connections

Skip connections represent the key innovation in UNet, enabling precise localization while preserving contextual
information.

###### The Localization Challenge

A fundamental challenge in segmentation architectures is balancing two competing needs:

1. **Context understanding**: Requires large receptive fields and deep networks, typically achieved through downsampling
2. **Precise localization**: Requires high-resolution feature maps to delineate exact boundaries

Without skip connections, upsampling alone struggles to recover the precise spatial details lost during downsampling,
resulting in coarse segmentation boundaries.

###### Skip Connection Mechanism

UNet's skip connections directly link corresponding layers between the contracting and expansive paths:

1. Feature maps from an encoder level are copied and concatenated with the upsampled feature maps in the corresponding
   decoder level
2. The concatenation occurs along the channel dimension, effectively combining features at the same resolution
3. This creates a composite feature map that contains both semantic context from deeper layers and spatial detail from
   earlier layers

These connections form information bridges across the architecture, allowing high-resolution details to flow directly to
the decoder.

###### Implementation Details

In practical UNet implementations, skip connections involve:

1. Storing feature maps from each encoder level before max pooling
2. After each upsampling step in the decoder, concatenating these stored features with the upsampled ones
3. Adjusting channel dimensions in the subsequent convolutions to process the expanded feature set

The concatenation operation preserves all information from both paths rather than using addition or other merging
operations.

###### Benefits of Skip Connections

Skip connections provide several critical advantages:

1. **Detail preservation**: Fine details lost during downsampling can be recovered during upsampling
2. **Gradient flow**: Direct paths from later to earlier layers improve gradient flow during training
3. **Feature reuse**: Early low-level features (edges, textures) remain directly accessible to later layers
4. **Multi-scale awareness**: The network can leverage information from multiple scales simultaneously

These benefits collectively enable UNet to produce segmentations with sharp, accurate boundaries—a critical requirement
for applications like medical imaging where precision directly impacts diagnostic accuracy.

###### Variations in Skip Connection Design

Since UNet's introduction, various skip connection designs have emerged:

1. **Feature selection**: Applying attention mechanisms to selectively emphasize important features
2. **Residual skip connections**: Adding residual connections within each level for improved gradient flow
3. **Dense skip connections**: Connecting each decoder level to all previous encoder levels
4. **Feature transformation**: Applying additional convolutions to encoder features before concatenation

These variations retain the core concept while addressing specific limitations or enhancing particular aspects of the
original design.

##### Dice Loss

The Dice Loss function addresses key challenges in segmentation training, particularly for imbalanced class
distributions common in medical and natural images.

###### Class Imbalance Problem

Semantic segmentation often faces extreme class imbalance:

1. In medical images, pathological regions might occupy <1% of the image
2. In natural scenes, certain classes (e.g., rare objects) may appear infrequently
3. With standard losses like cross-entropy, majority classes can dominate the gradient

This imbalance can lead models to predict only the dominant class, achieving high accuracy but failing on the critical
minority regions.

###### Dice Coefficient Fundamentals

The Dice coefficient (also known as the F1 score for binary cases) measures overlap between two sets:

$$\text{Dice} = \frac{2|X \cap Y|}{|X| + |Y|}$$

Where:

- X and Y are the two sets (predicted and ground truth segmentation)
- |X| denotes the size of set X
- |X ∩ Y| denotes the size of the intersection between X and Y

This coefficient ranges from 0 (no overlap) to 1 (perfect overlap).

###### Soft Dice Coefficient

To make the Dice coefficient differentiable for neural network training, a "soft" version is used:

$$\text{Dice}*{\text{soft}} = \frac{2\sum*{i=1}^{N} p_i g_i}{\sum_{i=1}^{N} p_i + \sum_{i=1}^{N} g_i}$$

Where:

- $p_i$ is the predicted probability for pixel i
- $g_i$ is the ground truth for pixel i (typically 0 or 1)
- N is the total number of pixels

This soft version accepts probability predictions rather than requiring hard binary decisions.

###### Dice Loss Formulation

The Dice Loss is simply defined as:

$$\text{Dice}*{\text{loss}} = 1 - \text{Dice}*{\text{soft}}$$

This creates a loss function that:

1. Equals 0 for perfect prediction
2. Approaches 1 for poor prediction
3. Can be used directly with gradient-based optimization

###### Relationship to F1 Score

The Dice coefficient can be interpreted in terms of precision and recall:

$$\text{Dice} = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2 \times \text{TP}}{2 \times \text{TP} + \text{FP} + \text{FN}}$$

Where:

- TP = True Positives
- FP = False Positives
- FN = False Negatives

This connection explains why Dice loss naturally balances precision and recall, unlike metrics like accuracy which can
be misleading with imbalanced classes.

###### Implementation Considerations

When implementing Dice Loss, several practical considerations improve stability:

1. **Smoothing factor**: Adding a small ε to both numerator and denominator prevents division by zero and stabilizes
   gradients:

$$\text{Dice}*{\text{loss}} = 1 - \frac{2\sum*{i=1}^{N} p_i g_i + \epsilon}{\sum_{i=1}^{N} p_i + \sum_{i=1}^{N} g_i + \epsilon}$$

1. **Multi-class adaptation**: For multiple classes, the Dice loss is typically calculated per class and then averaged:

$$\text{Dice}*{\text{multi}} = 1 - \frac{1}{C}\sum*{c=1}^{C}\text{Dice}_{c}$$

1. **Weighting strategies**: Classes can be weighted differently to further address imbalance

###### Dice Loss Advantages

In comparison to standard cross-entropy loss, Dice loss offers several benefits:

1. **Class balance**: Inherently addresses class imbalance without explicit class weighting
2. **Boundary emphasis**: Naturally emphasizes boundary regions where precision/recall metrics are most affected
3. **Direct optimization**: Optimizes the same metric that's often used for evaluation
4. **Scale invariance**: Not affected by the absolute number of pixels in each class

These advantages make Dice loss particularly well-suited for medical image segmentation, where precise delineation of
small structures is often critical.

###### Dice Loss Limitations

Despite its advantages, Dice loss has some limitations:

1. It can be unstable early in training when predictions are nearly random
2. It may struggle with very small structures where even slight misalignments drastically affect the coefficient
3. It doesn't account for spatial relationships between misclassified pixels

These limitations have led to hybrid approaches that combine Dice loss with cross-entropy or other loss functions.

The combination of UNet architecture with skip connections and Dice loss has revolutionized semantic segmentation,
particularly in medical imaging but increasingly across domains. This approach effectively balances the competing
requirements of contextual understanding and precise localization, while training reliably even with imbalanced class
distributions. Understanding these components provides insight into not just how semantic segmentation works, but why
certain architectural choices prove so effective for pixel-wise prediction tasks.

––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––

In this lesson we will focus on object localization, object detection and semantic segmentation. In this lesson we are
going to look at a core set of concepts and solutions that will help you get started on these fascinating topics, with
no intention of being complete. In particular we are going to focus on two workhorse architectures for object detection
and semantic segmentation, respectively: _RetinaNet_ and _UNet._ The core computer vision tasks and their objectives
are:

| Task                  | Objective                                                                                             |
| --------------------- | :---------------------------------------------------------------------------------------------------- |
| Image classification  | Assign one or more labels to an image                                                                 |
| Object localization   | Assign a label to most prominent object, define a box around that object                              |
| Object detection      | Assign a label and define a box for all objects in an image                                           |
| Semantic segmentation | Determine the class of each _pixel_ in the image                                                      |
| Instance segmentation | Determine the class of each _pixel_ in the image distinguishing different instances of the same class |

Image classification is the simplest task, providing only image-level labels and object localization and detection
involve both classification and spatial information (bounding boxes). The segmentation tasks operate at the pixel
level - semantic segmentation treats all instances of a class the same and Instance segmentation differentiates between
individual instances of the same class.

#### Object Localization

Object localization combines classification with spatial information, identifying both what an object is and where it is
located in an image. It has multi-head architecture with components:

1. _Backbone_: Shared feature extractor
2. _Classification Head_: Class prediction
3. _Localization Head_: Bounding box prediction

The Python implementation is provided below:

```python
class MultiHead(nn.Module):
    def __init__(self):
        super().__init__()
        self.backbone = nn.Sequential(...)  # Feature extraction
        self.class_head = nn.Sequential(...)  # Classification
        self.loc_head = nn.Sequential(...)  # Localization

    def forward(self, x):
        x = self.backbone(x)
        class_scores = self.class_head(x)
        bounding_box = self.loc_head(x)
        return class_scores, bounding_box
```

<br>
<br>

#### Loss Function

$L = CE(\hat{p}) + \alpha MSE(b, \hat{b})$

Where :

- $L$ is the total loss
- $CE(\hat{p})$ is Cross-Entropy loss for classification
- $MSE(b, \hat{b})$ is Mean Squared Error for bounding box
- $\alpha$ is a scaling hyperparameter

<br>

The expanded form is below:

$L = -\log \hat{p} + \alpha \sum_{i=1}^{4} (b_i - \hat{b_i})^2$

Where:

- $\hat{p}$ is predicted class probability

- $b_i$ are true bounding box coordinates

- $\hat{b_i}$ are predicted bounding box coordinates

- Sum is over 4 bounding box coordinates

The Key Considerations are:

1. _Loss Balancing:_

    - $\alpha$ balances classification and localization losses
    - Different scales may require different $\alpha$ values

2. _Bounding Box Format:_

    - Four coordinates required: $(x_{min}, y_{min}, x_{max}, y_{max})$ or
    - Center format: $(x_{center}, y_{center}, width, height)$

3. _Performance Metrics:_
    - Classification accuracy
    - Bounding box IoU (Intersection over Union)
    - Combined performance metrics

This multi-head approach allows simultaneous optimization of both classification and localization tasks through a
unified loss function, making it effective for object localization tasks.

<br>

#### Object Localization

Object Localization and Bounding Boxes Object localization is the task of assigning a label and determining the bounding
box of an object of interest in an image. A bounding box is a rectangular box that completely encloses the object, whose
sides are parallel to the sides of the image. The architecture of an object localization network is similar to the
architecture of a classification network, but we add one more head (the localization head) on top of the existing
classification head:

<p align="center">
<img src="images/obj-loc.png" alt="Object Localization Architecture" width="600" height=auto>
</p>
<p align="center">figure: Multi-head architecture for object localization showing backbone and two heads</p>

A multi-head model is a CNN where we have a backbone as typical for CNNs, but more than one head. For example, for
object localization this could look like:

```python
from torch import nn


class MultiHead(nn.Module):
    def __init__(self):
        super().__init__()
        # Backbone: this can be a custom network, or a
        # pre-trained network such as a resnet50 where the
        # original classification head has been removed. It computes
        # an embedding for the input image
        self.backbone = nn.Sequential(..., nn.Flatten())

        # Classification head: an MLP or some other neural network
        # ending with a fully-connected layer with an output vector
        # of size n_classes
        self.class_head = nn.Sequential(..., nn.Linear(out_feature, n_classes))

        # Localization head: an MLP or some other neural network
        # ending with a fully-connected layer with an output vector
        # of size 4 (the numbers defining the bounding box)
        self.loc_head = nn.Sequential(..., nn.Linear(out_feature, 4))

    def forward(self, x):
        x = self.backbone(x)

        class_scores = self.class_head(x)
        bounding_box = self.loc_head(x)

        return class_scores, bounding_box
```

#### Loss in a Multi-Head Model

An object localization network, with its two heads, is an example of a multi-head network. Multi-head models provide
multiple outputs that are in general paired with multiple inputs. For example, in the case of object localization, we
have 3 inputs as ground truth: the image, the label of the object ("car") and the bounding box for that object (4
numbers defining a bounding box). The object localization network processes the image through the backbone, then the two
heads provide two outputs.

The class scores, that need to be compared with the input label - predicted bounding box, that needs to be compared with
the input bounding box. The comparison between the input and predicted labels is made using the usual Cross-entropy
loss, while the comparison between the input and the predicted bounding boxes is made using, for example, the mean
squared error loss. The two losses are then summed to provide the total loss L.

<p align="center">
<img src="images/slide.png" alt="Sliding Window Approach" width="600" height=auto>
</p>
<p align="center">figure: Sliding window approach for object detection</p>

Since the two losses might be on different scales, we typically add a hyperparameter α to rescale one of the two:

$L = CE(\hat{p}) + \alpha MSE(b, \hat{b})$

Where:

$L$ is the total loss $CE(\hat{p})$ is the Cross-Entropy loss for classification $MSE(b, \hat{b})$ is the Mean Squared
Error for bounding box prediction $\alpha$ is a hyperparameter to balance the two losses. The Expanded Loss Formula is:

$L = -\log \hat{p} + \alpha \sum_{i=1}^{4} (b_i - \hat{b_i})^2$

Where:

- $\hat{p}$ is the predicted class probability
- $b_i$ are the true bounding box coordinates
- $\hat{b_i}$ are the predicted bounding box coordinates

<br>
<br>

#### Object Detection

The task of object detection consists of detecting and localizing all the instances of the objects of interest.
Different images of course can have a different number of objects of interest: for example an image could have one car,
zero cars, or many cars. The same images could also have one person, zero people or many people. In each one of these
cases, we need the network to output one vector of class scores plus 4 numbers to define the bounding box for each
object.

It is clear that a network with the same structure as the object localization network would not work, because we would
need a variable number of heads depending on the content of the image. One way would be to slide a window over the
image, from the upper left corner to the lower right corner, and for each location of the image we run a normal object
localization network. This sliding window approach works to a certain extent, but is not optimal because different
objects can have different sizes and aspect ratios. Thus, a window with a fixed size won't fit well all objects of all
sizes. For example, let's consider cars: depending on how close or far they are, their size in the image will be
different. Also, depending on whether we are seeing the back or the front, or the side of the car, the aspect ratio of
the box bounding the object would be pretty different. This becomes even more extreme if we consider objects from
different classes, for example cars and people: Nowadays there are two approaches to solving the problem of handling a
variable number of objects, and of their different aspect ratios and scales:

1. _One-stage object detection:_ We consider a fixed number of windows with different scales and aspect ratios, centered
   at fixed locations (anchors). The output of the network then has a fixed size. The localization head will output a
   vector with a size of 4 times the number of anchors, while the classification head will output a vector with a size
   equal to the number of anchors multiplied by the number of classes.

2. _Two-stage object detection:_ In the first stage, an algorithm or a neural network proposes a fixed number of windows
   in the image. These are the places in the image with the highest likelihood of containing objects. Then, the second
   stage considers those and applies object localization, returning for each place the class scores and the bounding
   box.

In practice, the difference between the two is that while the first type has fixed anchors (fixed windows in fixed
places), the second one optimizes the windows based on the content of the image. In this lesson we are only going to
treat one-stage object detection.

#### One-Stage Object Detection

The RetinaNet network is an example of a one-stage object detection algorithm. Like many similar algorithms, it uses
anchors to detect objects at different locations in the image, with different scales and aspect ratios. Anchors are
windows with different sizes and different aspect ratios, placed in the center of cells defined by a grid on the image:

We divide the image with a regular grid. Then for each grid cell we consider a certain number of windows with different
aspect ratios and different sizes. We then "anchor" the windows in the center of each cell. If we have 4 windows and 45
cells, then we have 180 anchors.

We run a localization network considering the content of each anchor. If the class scores for a particular class are
high for an anchor, then we consider that object detected for that anchor, and we take the bounding box returned by the
network as the localization of that object. This tends to return duplicated objects, so we post-process the output with
an algorithm like Non Maximum Suppression

#### Feature Pyramid Networks

RetinaNet uses a special backbone called Feature Pyramid Network, FPNs. The Feature Pyramid Network is an architecture
that extracts multi-level, semantically-rich feature maps from an image:

<p align="center">
<img src="images/fpn.png" alt="Feature Pyramid Network" width="600" height=auto>
</p>
<p align="center">figure: Feature Pyramid Network (FPN) architecture for multi-scale feature extraction</p>

A regular CNN backbone of convolution, pooling, and other typical CNN layers is used to extract multiple feature maps
from the original image (downsampling path). The last feature map contains the most semantic-rich representation, which
is also the least detailed because of the multiple pooling. So, we copy that to the upsampling path and run object
detection with anchors on that. Then, we upsample it and sum it to the feature map from the same level in the
downsampling path. This means we are mixing the high-level, abstract information from the feature map in the upsampling
path to the more detailed view from the downsampling path. We then run object detection on the result. We repeat this
operation several times (3 times in total in this diagram). This is how RetinaNet uses the Feature Pyramid Network:

<p align="center">
<img src="images/retina_net.png" alt="RetinaNet Architecture" width="600" height=auto>
</p>
<p align="center">figure: RetinaNet architecture combining FPN with focal loss for object detection</p>

RetinaNet conducts object classification and localization by independently employing anchors at each Feature Pyramid
level. The diagram above illustrates that the classification subnet and the box regression subnet pass through four
convolutional layers with 256 filters. Subsequently, they undergo convolutional layers with KA and 4A filters for the
classification and localization, respectively.

#### Focal Loss

The third innovative feature of RetinaNet is the so-called Focal Loss. When using a lot of anchors on multiple feature
maps, RetinaNet encounters a significant class balance problem: most of the tens of thousands of anchors used in a
typical RetinaNet will not contain objects. The crop of the image corresponding to these anchors will normally be pretty
easy to classify as background. So the network will very quickly become fairly confident on the background. The normal
cross-entropy loss assigns a low but non-negligible loss even to well-classified examples. For example, let's look at
the blue curve here:

<p align="center">
<img src="images/focal_loss.png" alt="Focal Loss" width="600" height=auto>
</p>
<p align="center">figure: Focal Loss function for addressing class imbalance in object detection</p>

Here we are considering for simplicity a binary classification problem. Let's consider an example having a positive
label. If our network has a confidence on the positive label of 0.8, it means it is classifying this example pretty
well.

it is assigning the right label, with a good confidence of 0.8. However, the loss for this example is still around 0.5.
Let's now consider a different positive example, where the network is assigning a probability for the positive class of
0.1. This is a False Negative, and the loss accordingly is pretty high (around 4). Let's now assume that we have 10
examples where the network is correct and has a confidence of 0.8 (and loss of 0.5), and one example where the network
is wrong and has a loss of 4. The ten examples will have a cumulative loss of 0.5 x 10 = 5, which is larger than 4. In
other words, the cumulative loss of the examples that are already classified well is going to dominate over the loss of
the example that is classified wrong. This means that the backpropagation will try to make the network more confident on
the 10 examples it is already classifying well, instead of trying to fix the one example where the network is wrong.
This has catastrophic consequences for networks like RetinaNet, where there are usually tens of thousands of easy
background anchors for each anchor containing an object.

The Focal Loss adds a factor in front of the normal cross-entropy loss to dampen the loss due to examples that are
already well-classified so that they do not dominate. This factor introduces a hyperparameter γ: the larger γ, the more
the loss of well-classified examples is suppressed. Summarizing, RetinaNet is characterized by three key features:

1. Anchors
2. Feature Pyramid Networks
3. Focal loss

<br>
<br>

#### Object Detection Metrics

##### Precision

Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. It
measures the accuracy of positive predictions and focus on quality of positive predictions. Of all the items we
predicted as positive, what fraction were actually positive is precision and defined as:

$Precision = \frac{True \space Positives}{True \space Positives + False \space Positives}$

In a spam email detection system:

- Precision = (Correctly identified spam emails) / (Total emails identified as spam)
- High precision means low false alarm rate

##### Recall

Recall is the ratio of correctly predicted positive observations to all actual positive observations. Of all the actual
positive items, what fraction did we predict correctly is recall. It measures completeness of positive predictions and
focus on coverage of positive cases. The recall is defined as:

$Recall = \frac{True \space Positives}{True \space Positives + False \space Negatives}$

In a disease detection system:

- Recall = (Correctly identified sick patients) / (Total actually sick patients)
- High recall means few missed positive cases

The trade-off between precision and recall are:

- Often inverse relationship between precision and recall
- Improving one typically decreases the other
- Balance depends on application needs

The choice between optimizing for precision vs. recall depends on the relative costs of false positives vs. false
negatives in your specific application.

##### True Positives (TP)

- Cases where model predicted YES, and actual was YES
- Example: Predicted cancer when cancer was present
- Formula contribution: Correctly identified positive cases

##### True Negatives (TN)

- Cases where model predicted NO, and actual was NO
- Example: Predicted no cancer when cancer was absent
- Formula contribution: Correctly identified negative cases

##### False Positives (FP) - Type I Error

- Cases where model predicted YES, but actual was NO
- Example: Predicted cancer when cancer was absent
- Also known as "False Alarm" or "Type I Error"

##### False Negatives (FN) - Type II Error

- Cases where model predicted NO, but actual was YES
- Example: Predicted no cancer when cancer was present
- Also known as "Miss" or "Type II Error"

##### Confusion Matrix

A Confusion Matrix is a performance evaluation tool in machine learning that shows the actual vs. predicted
classifications in a tabular format. For binary classification, it's typically a 2x2 matrix showing:

True Positives (TP): Correctly predicted positive cases True Negatives (TN): Correctly predicted negative cases False
Positives (FP): Incorrectly predicted positive cases (Type I error) False Negatives (FN): Incorrectly predicted negative
cases (Type II error)

From these values, we can calculate important metrics:

**Accuracy** = (TP + TN) / (TP + TN + FP + FN) **Precision** = TP / (TP + FP) **Recall** = TP / (TP + FN) **F1 Score** =
2 × (Precision × Recall) / (Precision + Recall)

Here's how a typical confusion matrix looks:

|                   | Predicted Positive   | Predicted Negative   |
| ----------------- | -------------------- | -------------------- |
| Actually Positive | True Positives (TP)  | False Negatives (FN) |
| Actually Negative | False Positives (FP) | True Negatives (TN)  |

This tool is particularly valuable because it gives a complete picture of model performance, revealing not just overall
accuracy but also specific types of errors the model makes. This can be crucial in applications where certain types of
errors are more costly than others, like medical diagnosis or fraud detection.

##### Precision (Positive Predictive Value)

$Precision = \frac{TP}{TP + FP}$

- Focus on prediction quality
- "When we predict positive, how often are we right?"

##### Recall (Sensitivity)

$Recall = \frac{TP}{TP + FN}$

- Focus on finding all positives
- "What proportion of actual positives do we catch?"

##### Specificity

$Specificity = \frac{TN}{TN + FP}$

- Focus on negative case identification
- "What proportion of actual negatives do we identify?"

##### Accuracy

$Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$

- Overall correctness
- "What proportion of all predictions are correct?"

##### F1 Score

$F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}$

- Harmonic mean of precision and recall

- Single score balancing both metrics

The improving metrics between precision and recall are:

1. _To Improve Precision:_

    - Increase prediction threshold
    - Reduce false positives
    - More conservative predictions

2. _To Improve Recall:_
    - Lower prediction threshold
    - Reduce false negatives
    - More liberal predictions

#### Impact on Model Evaluation

##### High Stakes Scenarios

- Medical Diagnosis:
    - FN could mean missing a disease
    - FP could mean unnecessary treatment
- Fraud Detection:
    - FN means missing fraud
    - FP means flagging legitimate transactions

##### Cost Considerations

- FP Cost vs FN Cost
- Business Impact
- Resource Allocation

#### Improving Different Metrics

##### To Reduce False Positives

- Increase prediction threshold
- More conservative model
- Better feature selection

##### To Reduce False Negatives

- Lower prediction threshold
- More sensitive model
- Additional relevant features

##### Balance Considerations

- Domain-specific requirements
- Cost-benefit analysis
- Regulatory requirements

#### Intersection over Union (IoU)

The IoU is a measure of how much two boxes (or other polygons) coincide. As the name suggests, it is the ratio between
the area of the intersection, or overlap, and the area of the union of the two boxes or polygons: IoU is a fundamental
concept useful in many domains, and is a key metric for the evaluation of object detection algorithms.

<p align="center">
<img src="images/bird.png" alt="Bird Detection Example" width="600" height=auto>
</p>
<p align="center">figure: Example of bird detection showing bounding box and IoU calculation</p>

<br>
<br>

#### Mean Average Precision

Mean Average Precision (mAP) conveys a measurement of precision averaged over the different object classes.

$mAR = \frac{\sum_{i=1}^{K} AR_i}{K}$

This appears to be the formula for mean Average Recall (mAR), where:

- $mAR$ is the mean Average Recall
- $\sum_{i=1}^{K}$ represents the summation from i=1 to K
- $AR_i$ represents the Average Recall for the i-th class/category
- $K$ is the total number of classes/categories

The formula calculates the mean of Average Recall values across K different classes or categories by summing up all AR
values and dividing by K.

Let’s say we have a number of classes. We consider all the binary classification problems obtained by considering each
class in turn as positive and all the others as negative. For each one of these binary sub-problems, we start by drawing
the _precision-recall curve_ that is obtained by measuring precision and recall for different confidence level
thresholds, while keeping the IoU threshold fixed (for example at 0.5). The confidence level is the classification
confidence level, i.e., the maximum of the softmax probabilities coming out of the classification head. For example, we
set the confidence threshold to 0.9 and measure precision and recall, then we change the threshold to say 0.89 and
measure precision and recall, and so on, until we get to a threshold of 0.1. This constitutes our precision-recall
curve:

We then interpolate the precision and recall curve we just obtained by using a monotonically-decreasing interpolation
curve, and we take the area under the curve. This represents the so-called Average Precision (AP) for this class:

We repeat this procedure for all classes, then we take the average of the different APs and call that mean Average
Precision, or mAP. A related metric is called mean Average Recall (mAR). Similarly to the mAP, we split our problem into
a number of binary classification problems. For each class, we compute the recall curve obtained by varying this time
the IoU threshold from 0.5 to 1. We can now consider the integral of the curve. Since we integrate between 0.5 and 1,
and Recall is a quantity bounded between 0 and 1, the integral would be bounded between 0 and 0.5. We therefore multiply
by 2 to make it a quantity bounded between 0 and 1. Twice the area under the recall curve represents the so-called
Average Recall (AR) for this class and We then take the average of the AR over the different classes, to define the mean
Average Recall, or mAR.

#### Questions and Answers

##### Q#1: What are the components of a CNN-based object localization model? (Select all that apply)

Answer: Three components apply:

- A CNN backbone
- A classification head
- A localization head

Explanation: A CNN-based object localization model consists of three main components:

1. CNN backbone: Extracts features from the input image
2. Classification head: Determines what the object is
3. Localization head: Predicts the bounding box coordinates The self-attention head is not a standard component of
   object localization models.

##### Q#2: Match the two types of object detection algorithms with their definition.

Answer:

- One-stage object detection: The algorithm considers all at once a fixed number of windows with different scales and
  aspect ratios
- Two-stage object detection: First some regions are proposed, then objects are detected and localized in each one

One-stage and two-stage object detection algorithms follow different approaches to solving the same problem: how to
detect objects at different scales and at different locations in an image.

Explanation: These represent two main approaches to object detection:

- One-stage detectors (like YOLO) process the entire image in a single pass
- Two-stage detectors (like R-CNN) first propose regions of interest, then classify and refine them

##### Q#3: What are the main components of RetinaNet?

Answer:

- A Feature Pyramid Network
- A classification head
- A localization head
- A large numbers of anchors

Explanation: RetinaNet combines these components to achieve effective object detection:

1. CNN backbone extracts features
2. FPN creates multi-scale feature representations
3. Classification head for object class prediction
4. Localization head for bounding box prediction
5. Multiple anchors at different scales for better detection

##### Q#4: Match each concept with its proper description.

| Metric                       | Description                                       |
| ---------------------------- | ------------------------------------------------- |
| Average Precision (AP)       | Area under the precision-recall curve             |
| Mean Average Precision (mAP) | Average over all classes of the Average Precision |
| Average Recall (AR)          | Area under the recall vs IoU curve                |
| Mean Average Recall (mAR)    | Average over all classes of the Average Recall    |

Additional Notes:

- AP measures performance for a single class
- mAP measures overall model performance across all classes
- AR evaluates the model's ability to detect objects at different IoU thresholds
- mAR provides a holistic view of recall performance across all classes

Formulas:

- mAP = $\frac{1}{K}\sum_{i=1}^{K} AP_i$ where K is number of classes
- mAR = $\frac{1}{K}\sum_{i=1}^{K} AR_i$ where K is number of classes

These metrics are crucial for evaluating object detection models and comparing different approaches.

Answer:

- Area under the precision-recall curve → Average Precision
- Area under the recall vs IoU curve → Average Recall
- Average over all classes of the Average Precision → Mean Average Precision (mAP)
- Average over all classes of the Average Recall → Mean Average Recall (mAR)

Explanation: These metrics are used to evaluate object detection models:

- Average Precision (AP) measures the area under the precision-recall curve for each class
- Average Recall (AR) measures the area under the recall-IoU curve for each class
- mAP averages the AP across all classes
- mAR averages the AR across all classes The formula for mAR was previously shown: $mAR = \frac{\sum_{i=1}^{K} AR_i}{K}$

#### Image Segmentation

The task of semantic segmentation consists of classifying each pixel of an image to determine to which class it belongs.
Another technique of image segmentation We have briefly mentioned previously, but are not going to discuss further here,
another technique of image segmentation called _instance segmentation_.

##### UNet Semantic Segmentation

The UNet is a specific architecture for semantic segmentation. It has the structure of a standard autoencoder, with an
encoder that takes the input image and encodes it through a series of convolutional and pooling layers into a
low-dimensional representation.

Then the decoder architecture starts from the same representation and constructs the output mask by using transposed
convolutions. However, the UNet adds skip connections between the feature maps at the same level in the encoder and in
the decoder, as shown below:

<p align="center">
<img src="images/unet.png" alt="UNet Architecture" width="600" height=auto>
</p>
<p align="center">figure: UNet architecture for semantic segmentation with encoder-decoder structure</p>

In the decoder, the feature map coming from the decoder path is concatenated along the channel dimension with the
feature map coming from the encoder path. This means that the next transposed convolution layer has access to
information with high semantic content coming from the previous decoder layer, along with information with high detail
coming from the encoder path. The final segmentation mask is then a lot more detailed than what you would obtain with a
simple encoder-decoder architecture without skip connections between the encoder and the decoder.

<br>
<br>

##### The Dice Loss

There are a few different losses that one can use for semantic segmentation. One loss that tend to work well in practice
is called Dice loss, named after Lee Raymond Dice, who published it in 1945. Here is how Dice loss is calculated:

$\text{Dice loss} = 1 - \frac{2\sum_{i=1}^{n_{\text{pix}}} p_iy_i}{\sum_{i=1}^{n_{\text{pix}}}(p_i + y_i)}$

Where:

- $p_i$ represents the i-th pixel in the prediction mask
- $y_i$ represents the i-th pixel in the ground truth mask
- $n_{\text{pix}}$ is the total number of pixels in the image
- $\sum_{i=1}^{n_{\text{pix}}}$ indicates summation over all pixels

<br>
<br>

The Dice loss derives from the F1 score, which is the geometric mean of precision and recall. Consequently, the Dice
loss tends to balance precision and recall at the pixel level.

Key Points:

1. Used primarily in semantic segmentation tasks
2. Based on F1 score principles
3. Balances precision and recall
4. Particularly effective for pixel-level predictions

This loss function is especially useful in segmentation tasks where we need to compare predicted masks with ground truth
masks at the pixel level. We will use the implementation of UNet provided by the wonderful open-source library
segmentation_models for PyTorch. The library also implements the Dice loss. This is how you can define a UNet using this
library:

<br>
<br>

##### Derivation of the Dice Loss

Let's start with how we compute $TP$:

$TP = \sum_{i=1}^{n_{\text{pix}}} p_iy_i$

Since $y_i$ is either 0 or 1, the sum over all pixels is equivalent to the sum of the $p_i$ limited to pixels where
$y_i$ is equal to 1. The higher the $p_i$ is for this subset, the more the network is confident that the positive pixels
are indeed positive, the higher the "continuous" TP value will be. For the denominator of $F_1$ expression:

$2TP + FN + FP = \sum_{i=1}^{n_{\text{pix}}} p_i + \sum_{i=1}^{n_{\text{pix}}} y_i = \sum_{i=1}^{n_{\text{pix}}}(p_i +
y_i)$

The "continuous" version of the $F_1$ score - called the Dice coefficient - is:

$\text{Dice coeff} = F_1 = \frac{2\sum_{i=1}^{n_{\text{pix}}} p_iy_i}{\sum_{i=1}^{n_{\text{pix}}}(p_i + y_i)} = \frac{2TP}{2TP + FN + FP}$

Where:

- $TP$ represents True Positives
- $FN$ represents False Negatives
- $FP$ represents False Positives

Properties:

- Maximum when predicted probabilities $p_i$ are 1 for all pixels where $y_i = 1$ in ground truth
- Both numerator and denominator equal $2n_{\text{pix}}$ for perfect performance
- Perfect performance gives coefficient of 1

Finally, the Dice loss is defined as:

$\text{Dice loss} = 1 - F_1$

<br>

This forms the foundation for deriving the Dice Loss, which is essentially $1 - F_1$ when applied at the pixel level in
image segmentation tasks. The relationship to the previous Dice Loss formula can be understood when:

- True Positives correspond to matching pixels ($p_iy_i$)
- The denominator accounts for all predictions ($p_i$) and ground truth ($y_i$) pixels

This representation helps understand why Dice Loss is effective for semantic segmentation tasks, as it directly derives
from the F1 score's balanced treatment of precision and recall. Let's consider for simplicity the binary classification
problem, and let's imagine we want to segment a car in an image. In semantic segmentation we can define the True
Positives (TP), the False Negatives (FN), and the False Positives (FP) as illustrated by the following diagram, where
the gray mask is the ground truth mask for the car and the blue mask is the prediction from the model:

Here $p_i$ represents the class probability coming from the network, while $y_i$ is the ground truth. While the $p_i$
will be numbers between 0 and 1, the $y_i$ are either 0 (for the background class) or 1 for the foreground class.

Variables explained:

- $p_i$: Network output probabilities (continuous values between 0 and 1)
- $y_i$: Ground truth labels (binary values: 0 or 1)
    - 0: Background class
    - 1: Foreground class

<br>
<br>

Key points:

- Loss decreases as performance improves
- Equals 0 only with perfect network output
- Derived from $F_1$ score principles
- Suitable for gradient-based optimization
- Effective for semantic segmentation tasks

This loss function provides a continuous, differentiable measure that can effectively train segmentation models while
balancing precision and recall.
