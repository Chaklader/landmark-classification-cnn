# C-3: CNNs in Depth

<br>
<br>

#### Convolution on Color Images and Multiple Channels

When working with grayscale images, convolution is relatively straightforward - our kernels are 2D matrices that slide
across a 2D input image. However, color images introduce an additional dimension that fundamentally changes how
convolution works.

A color image typically has three channels (Red, Green, Blue), making it a 3D volume rather than a 2D matrix. For an RGB
image with dimensions 224×224, we actually have a 224×224×3 volume of data. This third dimension requires us to adapt
our convolutional approach.

The dimension of 224×224 is not a requirement for all CNNs but is actually a common input size used in many popular CNN
architectures. There's nothing inherently special about this specific dimension, and I should clarify that point.

The 224×224 size became a standard for several historical and practical reasons:

1. **ImageNet legacy**: Many influential CNN architectures (like VGG, ResNet, and AlexNet variants) were designed for
   the ImageNet competition, which standardized on 224×224 inputs. This created a ripple effect where subsequent models
   adopted the same dimensions for compatibility and comparison.
2. **Compromise between detail and computation**: This size provides enough detail to recognize complex objects while
   remaining computationally manageable, especially in the early days of deep learning when GPU memory was more limited.
3. **Divisibility properties**: 224 is divisible by several powers of 2 (224 = 7 × 32), which is convenient after
   multiple pooling operations that typically reduce dimensions by factors of 2.
4. **Transfer learning convenience**: Since many pre-trained models expect this input size, it became a de facto
   standard to simplify transfer learning workflows.

Images can actually be any dimension for CNN processing. Some architectures use 256×256, 299×299 (Inception), 384×384,
or even larger dimensions like 512×512 for high-resolution tasks. Modern fully convolutional networks can even handle
arbitrary input sizes.

<p align="center">
<img src="images/convolution_process.gif" alt="Convolution Process" width="350" height=auto>
</p>
<p align="center">figure:  Convolution on Color Images and Multiple Channels</p>

##### 3D Kernels for 3D Inputs

In color image convolution, our kernel expands from a simple k×k matrix to a $k \times k \times n_{\text{channels}}$
volume, where $n_{\text{channels}}$ matches the number of channels in the input (typically 3 for RGB images). Each
channel in the input has its own corresponding filter weights in the kernel.

When this 3D kernel is applied to a color image, the operation happens simultaneously across all channels. For each
position of the kernel:

1. We perform element-wise multiplication between each channel of the kernel and the corresponding channel of the image
   patch
2. We sum all these multiplications together (across all three channels)
3. The result is a single value in the output feature map

This process ensures that the convolution can detect patterns that involve relationships between color channels - for
example, a particular combination of red, green, and blue values that signifies a specific texture or feature.

##### Multiple Feature Maps: Increasing Depth

The power of CNNs comes from their ability to learn many different patterns simultaneously. To achieve this, we don't
use just one 3D kernel - we use multiple kernels (often 16, 32, 64, or more). Each kernel produces its own feature map,
focusing on different aspects of the input image.

If we have an RGB input image and apply 64 different 3D kernels, we'll get 64 feature maps as output. These feature maps
collectively form the output of the convolutional layer, creating a new 3D volume of data that serves as input to the
next layer.

##### Stacking Convolutional Layers: The Channel Evolution

In a multi-layer CNN, the number of channels typically increases as we go deeper into the network:

1. **First convolutional layer**: Takes in the original image (3 channels for RGB) and produces n₁ feature maps
   (e.g., 64)
2. **Second convolutional layer**: Takes in the previous layer's output (64 channels) and produces n₂ feature maps
   (e.g., 128)
3. **Third convolutional layer**: Takes in the previous layer's output (128 channels) and produces n₃ feature maps
   (e.g., 256)

This progression reflects a fundamental aspect of CNN design: as we go deeper, we increase the channel count
(representing more abstract features) while reducing the spatial dimensions (through pooling or strided convolutions).

##### Parameter Calculation for Color Convolutions

Understanding the number of parameters in a convolutional layer helps us gauge model complexity. For a layer with:

- n_k filters (producing n_k feature maps)
- kernel size k×k
- c input channels

The total number of parameters is: $$n_p = n_k(c \cdot k^2 + 1)$$

The term c·k² represents the weights in each 3D filter, and the +1 accounts for the bias term for each filter.

For example, a convolutional layer with 64 filters, each sized 3×3, operating on an RGB image (3 channels) would have:
$$n_p = 64 \cdot (3 \cdot 3^2 + 1) = 64 \cdot (27 + 1) = 64 \cdot 28 = 1,792$$ parameters.

This is remarkably efficient compared to a fully connected layer handling the same image data, which would require
millions of parameters.

##### Visualizing the Process

Imagine an RGB image as a stack of three 2D matrices (one for each color channel). When we apply a 3D convolutional
kernel, it's like having three separate 2D kernels - one for each channel. Each kernel interacts with its corresponding
channel, and then the results are summed to produce a single value in the output feature map.

This process repeats as the kernel slides across the entire image, and it happens in parallel for every filter in the
layer. The result is a set of feature maps that highlight different aspects of the input image, from simple edges in
early layers to complex patterns in deeper layers.

##### The Channel Terminology

In CNN discussions, the term "channels" has dual meaning:

1. The color channels of the input image (RGB)
2. The feature maps produced by convolutional layers

This shared terminology reinforces the conceptual similarity: both represent different "aspects" or "views" of the
visual information. As we progress through a CNN, the interpretation evolves from color channels to increasingly
abstract feature channels, but the mathematical treatment remains consistent.

Understanding this channel-based processing is key to grasping how CNNs transform simple pixel data into sophisticated
visual understanding.

##### Convolutional Layer Parameters

Let's see how we can compute the number of parameters in a convolutional layer, $n_p$. Let's define some quantities:

- $n_k$: number of filters in the convolutional layer
- k: height and width of the convolutional kernel
- c: number of feature maps produced by the previous layer (or number of channels in input image)

There are $k$ times $k$ times $c$ weights per filter plus one bias per filter, so $ck^2 + 1$ parameters. The
convolutional layer is composed of $n_k$ filters, so the total number of parameters in the convolutional layer is:

$n_p = n_k(ck^2 + 1)$.

<userStyle>Claude aims to give clear, thorough explanations that help the human deeply understand complex topics. Claude
approaches questions like a teacher would, breaking down ideas into easier parts and building up to harder concepts. It
uses comparisons, examples, and step-by-step explanations to improve understanding. Claude keeps a patient and
encouraging tone, trying to spot and address possible points of confusion before they arise. Claude may ask thinking
questions or suggest mental exercises to get the human more involved in learning. Claude gives background info when it
helps create a fuller picture of the topic. It might sometimes branch into related topics if they help build a complete
understanding of the subject. When writing code or other technical content, Claude adds helpful comments to explain the
thinking behind important steps. Claude always writes prose and in full sentences, especially for reports, documents,
explanations, and question answering. Claude can use bullets only if the user asks specifically for a list.</userStyle>

#### Stride and Padding

When designing Convolutional Neural Networks, we often need precise control over how the spatial dimensions (height and
width) change from layer to layer. Stride and padding are two fundamental parameters that give us this control, allowing
us to build networks with specific architectural characteristics.

##### Understanding Stride

Stride determines how far the convolutional kernel moves after each application. Think of stride as the "step size" of
the kernel as it slides across the input.

With a stride of 1 (the default), the kernel shifts by just one pixel at a time, creating an overlap between adjacent
applications. This produces an output feature map that's only slightly smaller than the input (reduced by kernel size -
1).

When we increase the stride to 2, the kernel jumps two pixels at a time, effectively skipping every other position. This
reduces the spatial dimensions of the output by approximately half. A stride of 3 would reduce dimensions by
approximately two-thirds, and so on.

Stride serves two important purposes in CNN design:

1. **Dimensional reduction**: Large strides allow us to reduce spatial dimensions without requiring pooling layers,
   making the network more computationally efficient
2. **Field of view control**: Larger strides effectively increase the receptive field size relative to the output size,
   allowing deeper layers to "see" more of the original input

##### The Role of Padding

Padding involves adding extra pixels around the border of an input image or feature map before applying convolution.
These added pixels form a "frame" around the original data.

Without padding, each convolution operation reduces the spatial dimensions of the feature maps. After several
convolutional layers, the feature maps can become too small, losing important information, especially at the edges.

Padding addresses this issue by preserving spatial dimensions and ensuring that pixels at the edges receive equal
attention in the convolution process.

##### Types of Padding

While the most common padding approach is zero-padding (adding zeros around the borders), several other strategies
exist:

1. **Zero padding**: All added border pixels are set to zero
2. **Reflect padding**: Border pixels mirror the nearest actual pixels (like a reflection)
3. **Replicate padding**: Border pixels copy the values of the nearest actual pixels
4. **Circular padding**: Pixels from the opposite side of the image wrap around to form the padding

Each approach has its strengths, but zero padding is most common due to its simplicity and effectiveness. The choice may
depend on the nature of your data and the specific patterns you're trying to detect.

##### The Output Size Formula

To calculate the exact output dimensions after convolution, we use this formula:

$$o = \left\lfloor\frac{i + 2p - k}{s}\right\rfloor + 1$$

Where:

- $o$ is the output size (height or width)
- $i$ is the input size (height or width)
- $p$ is the padding size
- $k$ is the kernel size
- $s$ is the stride
- $\lfloor...\rfloor$ represents the floor function (rounding down)

This formula helps us predict exactly how our architectural choices will affect feature map dimensions throughout the
network.

##### Common Padding Strategies

Two common padding approaches have emerged in CNN architecture design:

1. **Valid padding** (no padding): The kernel is only applied where it completely fits within the input. This always
   results in a smaller output.
2. **Same padding**: Padding is added so that the output dimensions match the input dimensions (when using stride=1).
   The amount of padding needed is calculated as: $$p = \frac{k - 1}{2}$$ This works perfectly when the kernel size is
   odd (3×3, 5×5, etc.), which is why odd-sized kernels are preferred in most CNN architectures.

##### Practical Examples

Let's explore how different combinations of stride and padding affect output dimensions:

**Example 1**: Input: 32×32, Kernel: 3×3, Stride: 1, Padding: 0

- Output: $\lfloor\frac{32 + 2(0) - 3}{1}\rfloor + 1 = 30$
- Result: 30×30 feature map (edges reduced)

**Example 2**: Input: 32×32, Kernel: 3×3, Stride: 1, Padding: 1

- Output: $\lfloor\frac{32 + 2(1) - 3}{1}\rfloor + 1 = 32$
- Result: 32×32 feature map (spatial dimensions preserved)

**Example 3**: Input: 32×32, Kernel: 3×3, Stride: 2, Padding: 1

- Output: $\lfloor\frac{32 + 2(1) - 3}{2}\rfloor + 1 = 16$
- Result: 16×16 feature map (halved dimensions)

##### Architectural Implications

The choices of stride and padding significantly impact network architecture and behavior:

1. **Networks without padding** tend to rapidly reduce spatial dimensions, potentially losing important edge information
   and limiting depth
2. **Networks with padding** can maintain spatial dimensions longer, allowing for deeper architectures and better
   preservation of edge features
3. **Strided convolutions** can replace pooling layers for downsampling, potentially learning more optimal ways to
   reduce dimensions

Modern CNN architectures typically use padding to maintain spatial information in early layers and carefully chosen
strides to gradually reduce dimensions in a controlled manner throughout the network.

#### Advanced Pooling Techniques

Pooling layers serve as dimensional reduction mechanisms within Convolutional Neural Networks, systematically condensing
the spatial information while preserving the most important features. Unlike convolutional layers that learn to extract
features, pooling operations follow fixed mathematical rules to summarize information.

Think of pooling as creating a lower-resolution version of each feature map - we're essentially saying, "We don't need
pixel-perfect detail; we just need to know if this feature exists in this general area." This information compression
provides several benefits crucial to CNN performance.

##### Max Pooling

Max pooling, the most widely used pooling technique, operates by taking the maximum value within each window. For
instance, with a 2×2 window, we examine four values and keep only the largest one:

```shell
| 3  7 |
| 4  9 |   →   | 9 |
```

This approach is particularly effective for feature detection because it preserves the strongest activation in each
region. If a certain feature (like an edge or texture) appears anywhere within the pooling window, its signal passes
through. This creates a form of translational invariance - the exact position of the feature within the window becomes
less important.

In practical terms, max pooling emphasizes the presence of features rather than their precise locations, allowing the
network to recognize objects even when they appear in slightly different positions within the image.

##### Average Pooling

Average pooling computes the mean of all values within each window. Using the same 2×2 example:

```shell
| 3  7 |
| 4  9 |   →   | 5.75 |
```

Unlike max pooling which focuses on the strongest activations, average pooling considers all values equally. This makes
it well-suited for capturing background textures, overall color patterns, or any feature where the collective response
matters more than individual peak activations.

Average pooling tends to produce smoother, more blended feature maps. While less common as a general-purpose pooling
method, it finds important applications in:

1. Later network stages where feature maps represent higher-level concepts
2. Global context modeling where overall patterns are more important than individual details
3. Networks focused on texture analysis or style transfer

##### Global Pooling

Global pooling extends the pooling concept to cover the entire feature map, reducing each map to a single value. The two
main variants are:

1. **Global Max Pooling**: Takes the maximum value from the entire feature map
2. **Global Average Pooling**: Computes the average of all values in the feature map

This technique has become increasingly popular in modern architectures as a replacement for flattening layers, offering
several advantages:

1. **Parameter efficiency**: Eliminates the need for large fully-connected layers
2. **Input size flexibility**: Networks can accept variable-sized input images
3. **Regularization effect**: Reduces overfitting by forcing the network to focus on global patterns
4. **Interpretability**: Each value directly corresponds to the presence of a specific high-level feature

Global Average Pooling (GAP) has become particularly important in classification networks like ResNet, where it serves
as the final feature extraction step before classification.

##### Specialized Pooling Variations

Beyond the standard techniques, several specialized pooling methods have emerged for specific applications:

1. **Spatial Pyramid Pooling**: Performs pooling at multiple scales and concatenates the results, allowing the network
   to capture features at different levels of detail
2. **Fractional Max Pooling**: Uses non-integer ratios for downsampling, enabling finer control over dimension reduction
3. **Mixed Pooling**: Combines max and average pooling, often with a learnable weighting parameter to determine their
   relative importance
4. **Stochastic Pooling**: Randomly selects values within each pooling window based on their magnitude, introducing a
   form of regularization

##### Benefits of Pooling

Beyond its feature extraction capabilities, pooling dramatically reduces computational requirements in several ways:

1. **Dimensionality reduction**: A 2×2 pool with stride 2 reduces feature map area by 75%, correspondingly reducing
   computation in subsequent layers
2. **Memory efficiency**: Smaller feature maps require less memory storage during training and inference
3. **Receptive field expansion**: Each neuron in post-pooling layers effectively "sees" a larger portion of the original
   input

This efficiency allows networks to go deeper with the same computational budget, enabling more complex feature
hierarchies.

##### Pooling vs. Strided Convolutions

In recent years, some architectures have begun replacing pooling layers with strided convolutions, where the convolution
operation itself handles downsampling. This approach has some theoretical advantages:

1. The downsampling becomes learnable rather than following a fixed rule
2. The network can potentially discover more optimal ways to condense information
3. Architectural simplicity from using fewer types of layers

However, traditional pooling remains prevalent because of its simplicity, computational efficiency, and the useful
inductive bias it introduces into the network. Many state-of-the-art architectures continue to use pooling layers,
particularly max pooling, demonstrating their enduring value in CNN design.

##### Implementation Considerations

When implementing pooling in deep learning frameworks, several parameters require attention:

1. **Window size**: Typically 2×2 for gradual dimension reduction
2. **Stride**: Usually matches the window size for non-overlapping pooling
3. **Padding**: Rarely used with pooling layers but available for special cases

By understanding the various pooling techniques and their implications, you can make informed architectural decisions
that balance feature preservation, computational efficiency, and the specific requirements of your computer vision task.

#### CNN Structure

In a typical CNN there are several convolutional layers intertwined with Max Pooling layers. The convolutional layers
have more and more feature maps as you go deeper into the network, but the size of each feature map gets smaller and
smaller thanks to the Max Pooling layer.

This kind of structure goes hand in hand with the intuition we have developed in another lesson: as the signal goes
deeper into the network, more and more details are dropped, and the content of the image is "abstracted." In other
words, while the initial layers focus on the constituents of the objects (edges, textures, and so on), the deeper layers
represent and recognize more abstract concepts such as shapes and entire objects.

The convolution part of a CNN is implemented in PyTorch by using the _nn.Conv2d_ layer for convolution and the
_nn.MaxPool2d_ layer for max pooling. Stacking different blocks of convolution followed by pooling constitutes the
typical structure of a simple CNN. Typically the sizes of the feature maps shrink as you go deeper into the network,
while the channel count (i.e., the number of feature maps and filters) increases going deeper into the network, as shown
below.

<br>
<p align="center">
<img src="images/detailed.png" alt="Detailed CNN Structure" width="600" height=auto>
</p>
<p align="center">figure: A detailed structure of a typical CNN</p>

The backbone is made of convolutional and pooling layers, and has the task of extracting information from the image.
After the backbone there is a flattening layer that takes the output feature maps of the previous convolutional layer
and flattens them out in a 1d vector: for each feature map the rows are stacked together in a 1d vector, then all the 1d
vectors are stacked together to form a long 1d vector called a _feature vector_ or _embedding_. This process is
illustrated by the following image:

<br>
<p align="center">
<img src="images/detailed_1.png" alt="Feature Map Flattening" width="600" height=auto>
</p>
<p align="center">figure: Flattening of feature maps into a 1d vector</p>

<br>
<p align="center">
<img src="images/detailed_2.png" alt="Feature Vector Formation" width="600" height=auto>
</p>
<p align="center">figure: Formation of the feature vector</p>

#### Convolutional Block in PyTorch

The typical sequence convolution -> pooling -> activation (with optional dropout) can be written in PyTorch like this:

```python
self.conv1 = nn.Conv2d(3, 16, 3, padding=1),
self.pool = nn.MaxPool2d(2, 2),
self.relu1 = nn.ReLU()
self.drop1 = nn.Dropout2d(0.2)
```

Of course with the _nn.Sequential_ equivalent:

```python
self.conv_block = nn.Sequential(
    nn.Conv2d(3, 16, 3, padding=1),
    nn.MaxPool2d(2, 2),
    nn.ReLU(),
    nn.Dropout2d(0.2)
)
```

#### CNN using PyTorch

Let's now bring everything together and write our first CNN in PyTorch. We are going to have 3 convolutional blocks plus
a head with a simple MLP.

```python
import torch
import torch.nn as nn


class MyCNN(nn.Module):

    def __init__(self, n_classes):
        super().__init__()

        # Create layers. In this case just a standard MLP
        self.model = nn.Sequential(
            # First conv + maxpool + relu
            nn.Conv2d(3, 16, 3, padding=1),
            nn.MaxPool2d(2, 2),
            nn.ReLU(),
            nn.Dropout2d(0.2),

            # Second conv + maxpool + relu
            nn.Conv2d(16, 32, 3, padding=1),
            nn.MaxPool2d(2, 2),
            nn.ReLU(),
            nn.Dropout2d(0.2),

            # Third conv + maxpool + relu
            nn.Conv2d(32, 64, 3, padding=1),
            nn.MaxPool2d(2, 2),
            nn.ReLU(),
            nn.Dropout2d(0.2),

            # Flatten feature maps
            nn.Flatten(),

            # Fully connected layers. This assumes
            # that the input image was 32x32
            nn.Linear(1024, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, n_classes)
        )

    def forward(self, x):
        # nn.Sequential will call the layers
        # in the order they have been inserted
        return self.model(x)
```

<br>
<br>

Let's analyze what is going on in the Sequential call. We have a series of 3 convolutional parts constituted of a
convolutional layer, a max pooling operation that halves the input shape, and then a ReLU activation:

```python
nn.Conv2d(3, 16, 3, padding=1),
nn.MaxPool2d(2, 2),
nn.ReLU()
```

We can also optionally insert a _nn.Dropout2d_ layer for regularization. We repeat this structure 3 times, varying the
number of feature maps in the sequence 16 -> 32 -> 64. As we go deep, in other words, we are working with feature maps
with a smaller height and width (because we keep applying max pooling) but with a higher channel count. This is very
typical and helps the network with abstracting concepts.

Then, we have a Flatten layer that flattens our 64 feature maps (coming from the last conv layer before the flattening)
into one long vector. Assuming that the input is 32x32, this vector will contain 1024 (4x4x64) numbers. Finally, we have
an MLP made of fully-connected layers that combines all the information extracted by the convolutional part and outputs
one number for each class (logits). We first compress the 1024-long array into an embedding of 128 numbers, and then
from there to the number of classes we have. Since we have used the nn.Sequential class, the forward method is extremely
simple, and it is just calling that Sequential instance.

#### Optimizing the Performance of Our Network

Now that we have seen how to train a simple CNN, let’s dive deeper and see how we can improve on the performance of our
network with some widely-adopted tricks.

1. _Image augmentation_: The basic idea of image augmentation is the following: if you want your network to be
   insensitive to changes such as rotation, translation, and dilation, you can use the same input image and rotate it,
   translate it, and scale it and ask the network not to change its prediction! In practice, this is achieved by
   applying random transformations to the input images before they are fed to the network.

2. Image Augmentation Using Transformations

3. Batch Normalization

<br>
<br>

A typical training augmentation pipeline is represented in this diagram:

<br>
<p align="center">
<img src="images/aug.png" alt="Training Augmentation Pipeline" width="600" height=auto>
</p>
<p align="center">figure: A typical training augmentation pipeline</p>

<br>
<br>

```python
import torchvision.transforms as T

train_transforms = T.Compose(
    [
        # The size here depends on your application. Here let's use 256x256
        T.Resize(256),
        # Let's apply random affine transformations (rotation, translation, shear)
        # (don't overdo here!)
        T.RandomAffine(scale=(0.9, 1.1), translate=(0.1, 0.1), degrees=10),
        # Color modifications. Here I exaggerate to show the effect
        T.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),
        # Apply an horizontal flip with 50% probability (i.e., if you pass
        # 100 images through around half of them will undergo the flipping)
        T.RandomHorizontalFlip(0.5),
        # Finally take a 224x224 random part of the image
        T.RandomCrop(224, padding_mode="reflect", pad_if_needed=True),  # -
        T.ToTensor(),
        T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
    ]
)
```

<br>
<br>

#### Transformation Pipelines

<br>

During validation and test you typically do not want to apply image augmentation (which is needed for training). Hence,
this is a typical transform pipeline for validation and test that can be paired with the pipeline above:

```python
testval_transforms = T.Compose(
    [
        # The size here depends on your application. Here let's use 256x256
        T.Resize(256),
        # Let's take the central 224x224 part of the image
        T.CenterCrop(224),
        T.ToTensor(),
        T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
    ]
)
```

The resize and crop should be the same as applied during training for best performance and the normalization should be
the same between training and inference (validation and test)

#### AutoAugment Transforms

There is a special class of transforms defined in torchvision, referred to as _AutoAugment_. These classes implement
augmentation policies that have been optimized in a data-driven way, by performing large-scale experiments on datasets
such as ImageNet and testing many different recipes, to find the augmentation policy giving the best result. It is then
proven that these policies provide good performances also on datasets different from what they were designed for.

For example, one such auto-transform is called _RandAugment_ and it is widely used. It is particularly interesting
because it parametrizes the strength of the augmentations with one single parameter that can be varied to easily find
the amount of augmentations that provides the best results. This is how to use it:

<br>

```python
T.RandAugment(num_ops, magnitude)
```

The main parameters are:

1. _num_ops_: the number of random transformations applied. Defaut: 2
2. _magnitude_: the strength of the augmentations. The larger the value, the more diverse and extreme the augmentations
   will become.

<br>
<br>

#### Batch Normalization

The second modern trick that paves the way for enhancing the performance of a network is called _Batch Normalization_,
or BatchNorm. It does not usually improve the performances per say, but it allows for much easier training and a much
smaller dependence on the network initialization, so in practice it makes our experimentation much easier, and allows us
to more easily find the optimal solution.

Just as we normalize the input image before feeding it to the network, we would like to keep the feature maps
normalized, since they are the output of one layer and the input to the next layer. In particular, we want to prevent
them to vary wildly during training, because this would require large adjustments of the subsequent layers. _BatchNorm_
normalizes the activations and keep them much more stable during training, making the training more stable and the
convergence faster.

In order to do this, during training BatchNorm needs the mean and the variance for the activations for each mini-batch.
This means that the batch size cannot be too small or the estimates for mean and variance will be inaccurate. During
training, the BatchNorm layer also keeps a running average of the mean and the variance, to be used during inference.

During inference, we don't have mini-batches. Therefore, the layer uses the mean and the variance computed during
training (the running averages). This means that BatchNorm behaves differently during training and during inference. The
behavior changes when we set the model to training mode (using _model.train()_) or to validation mode (_model.eval()_).

<br>
<br>

Just as we normalize the input image before feeding it to the network, we would like to keep the feature maps
normalized, since they are the output of one layer and the input to the next layer. In particular, we want to prevent
them to vary wildly during training, because this would require large adjustments of the subsequent layers. BatchNorm
normalizes the activations and keep them much more stable during training, making the training more stable and the
convergence faster.

In order to do this, during training BatchNorm needs the mean and the variance for the activations for each mini-batch.
This means that the batch size cannot be too small or the estimates for mean and variance will be inaccurate. During
training, the BatchNorm layer also keeps a running average of the mean and the variance, to be used during inference.

During inference we don't have mini-batches. Therefore, the layer uses the mean and the variance computed during
training (the running averages). This means that BatchNorm behaves differently during training and during inference. The
behavior changes when we set the model to training mode (using _model.train()_) or to validation mode (_model.eval()_).
Batch Normalization is a technique used to improve the training of Convolutional Neural Networks (CNNs) by standardizing
the inputs to each layer.

$x \leftarrow \frac{x - \mu_x}{\sigma_x} \gamma + \beta$

Where:

- $x$: activations (values in the feature maps)
- $\mu_x$: mean of the activations
- $\sigma_x$: standard deviation of the activations
- $\gamma$: learnable scaling parameter
- $\beta$: learnable shifting parameter

The key components are:

1. _Normalization_: $(x - \mu_x) / \sigma_x$

    - Centers the activations around zero
    - Scales them to have unit variance

2. _Learnable Parameters_: $\gamma$ and $\beta$
    - Allow the network to undo normalization if needed
    - Provide flexibility in learning optimal activation distributions

##### BatchNorm Benefits

1. Reduces internal covariate shift
2. Allows higher learning rates, accelerating training
3. Acts as a form of regularization
4. Reduces the dependence on careful initialization

Please, mind the following important items:

- Applied after the linear transformation but before the activation function
- Uses _mini-batch_ statistics during training
- Uses population statistics during inference

- Batch size affects the stability of BatchNorm
- May require adjustments in very small batch sizes or online learning scenarios

Batch Normalization standardizes layer inputs, significantly improving training speed and stability in deep neural
networks, especially CNNs.

<br>
<br>

##### Pros and Cons of BatchNorm

| Pros                                                                             | Cons                                                             |
| -------------------------------------------------------------------------------- | ---------------------------------------------------------------- |
| Easily Added to Most Architectures                                               | Breaks with Small Batch Size (Mean and std dev become too noisy) |
| Faster Training (Larger learning rate, fewer epochs)                             | Cumbersome (Difference between train and inference)              |
| Reduce Sensitivity to Initialization                                             | Slow (Added computation at inference time)                       |
| Small Regularization Effect (Introduces a bit of noise through batch statistics) |                                                                  |

<br>

##### BatchNorm for Convolutional Layers

BatchNorm can be used very easily in PyTorch as part of the convolutional block by adding the _nn.BatchNorm2d_ layer
just after the convolution:

```python
self.conv1 = nn.Sequential(
    nn.Conv2d(3, 16, kernel_size=3, padding=1),
    nn.BatchNorm2d(16),
    nn.MaxPool2d(2, 2),
    nn.ReLU(),
    nn.Dropout2d(0.2)
)
```

<br>
<br>

The only parameter is the number of input feature maps, which of course must be equal to the output channels of the
convolutional layer immediately before it. NOTE: It is important to use BatchNorm before DropOut. The latter drops some
connections only at training time, so placing it before BatchNorm would cause the distribution seen by BatchNorm to be
different between training and inference.

##### BatchNorm for Dense Layers

We can add BatchNorm to MLPs very easily by using nn.BatchNorm1d:

```python
self.mlp = nn.Sequential(
    nn.Linear(1024, 500),
    nn.BatchNorm1d(500),
    nn.ReLU(),
    nn.Dropout(0.5)
)
```

#### Hyperparameters Optimization

Optimizing hyperparameters can be confusing at the beginning, so we provide you with some rules of thumb about the
actions that typically matter the most. They are described in order of importance below. They are not strict rules, but
should help you get started:

1. _Design parameters_: When you are designing an architecture from scratch, the number of hidden layers, as well as the
   layers parameters (number of filters, width and so on) are going to be important.

2. _Learning rate_: Once the architecture is fixed, this is typically the most important parameter to optimize. The next
   video will focus on this.

3. _Batch size_: This is typically the most influential hyperparameter after the learning rate. A good starting point,
   especially if you are using BatchNorm, is to use the maximum batch size that fits in the GPU you are using. Then you
   vary that value and see if that improves the performances.

4. _Regularization_: Once you optimized the learning rate and batch size, you can focus on the regularization,
   especially if you are seeing signs of overfitting or underfitting.

5. _Optimizers_: Finally, you can also fiddle with the other parameters of the optimizers. Depending on the optimizers,
   these vary. Refer to the documentation and the relevant papers linked there to discover what these parameters are.

The learning rate is one of the most important hyperparameters. However, knowing what the optimal value is, or even what
a good range is, can be challenging. One useful tool to discover a good starting point for the learning rate is the
so-called "learning rate finder." It scans different values of the learning rate, and computes the loss obtained by
doing a forward pass of a mini-batch using that learning rate.

```shell
argmax{M(argmin L(W|H))}
   H      W
```

Where:

- H is defined as the set of hyperparameters
- M is defined as the metric of interest
- W represents the weights of the model
- L represents the loss function

This formula describes a _bi-level optimization_ problem where:

1. _Inner optimization, W_: _argmin L(W|H)_

    - This finds the optimal weights W that minimize the loss function L, given some fixed hyperparameters H

2. _Outer optimization, H_: `argmax M(...)`
    - This finds the optimal hyperparameters H that maximize some metric M when evaluated on the model with optimized
      weights

This type of formula is commonly used in hyperparameter optimization or meta-learning scenarios, where you want to:

1. Train a model (inner optimization)
2. Evaluate its performance using some metric
3. Use that information to find the best hyperparameters (outer optimization)

<br>
<p align="center">
<img src="images/hp1.png" alt="Hyperparameter Tuning" width="600" height=auto>
</p>
<p align="center">figure: Hyperparameter Tuning</p>

<br>
<br>

##### Parameter

1. Internal to the model
2. May vary during training
3. Examples: Weights and biases of a network

##### Hyperparameter

1. External to the model
2. Fixed during training
3. Examples: Learning rate, number of layers, activation layers

##### Experiment

1. A specific training run with a fixed set of hyperparameters
2. Practitioners typically perform many experiments varying the hyperparameters. Each experiment produces one or more
   metrics that can be used to select the best-performing set of hyperparameters (see the next section).

<br>
<br>

#### Optimization Strategies

##### Grid search

1. Divide the parameter space in a regular grid
2. Execute one experiment for each point in the grid
3. Simple, but wasteful

##### Random search

1. Divide the parameter space in a random grid
2. Execute one experiment for each point in the grid
3. Much more efficient sampling of the hyperparameter space with respect to grid search

##### Bayesian Optimization

1. Algorithm for searching the hyperparameter space using a Gaussian Process model
2. Efficiently samples the hyperparameter space using minimal experiments

<p align="center">
<img src="images/hp2.png" alt="Hyperparameter Tuning" width="600" height=auto>
</p>
<p align="center">figure: Hyperparameter Tuning Strategies</p>

<br>
<br>

#### Learning Rate Schedulers

In many cases we want to vary the learning rate as the training progresses. At the beginning of the training we want to
make pretty large steps because we are very far from the optimum. However, as we approach the minimum of the loss, we
need to make sure we do not jump over the minimum.

<p align="center">
<img src="images/learning_rate.png" alt="Learning Rate Finder" width="600" height=auto>
</p>
<p align="center">figure:  Learning Rate Scheduler</p>

For this reason, it is often a good idea to use a learning rate scheduler, i.e., a class that changes the learning rate
as the training progresses. There are several possible learning rate schedulers. You can find the available ones in the
PyTorch learning rate schedulers documentation. One of the simplest one is the _StepLR_ scheduler. It reduces the
learning rate by a specific factor every n epochs. It can be used as follows:

```python
from torch.optim.lr_scheduler import StepLR

scheduler = StepLR(optimizer, step_size=5, gamma=0.5)

# Training loop
for ...
    ...
    # Update the weights
    optimizer.step()

    # Update the learning rate in the
    # optimizer according to the schedule
    scheduler.step()
```

<br>
<br>

#### Tracking Experiments

When you are performing hyperparameter optimization and other changes it is very important that you track all of your
experiments. This way you will know which hyperparameters have given you which results, and you will be able to repeat
those experiments, choose the best one, understand what works and what doesn't, and what you need to explore further.
You will also be able to present all your results to other people. Let's consider mlflow, which is free and open source.

Tracking an experiment is easy in mlflow. You first start by creating a run. A run is a unit of execution that will
contain your results. Think of it as one row in a hypothetical spreadsheet, where the columns are the things you want to
track ( accuracy, validation loss, ...). A run can be created like this:

```python
with mlflow.start_run():
```

Once you have created the run, you can use _mlflow.log_param_ to log a parameter (i.e., one of the hyperparameters for
example) and _mlflow.log_metric_ to log a result (for example the final accuracy of your model). For example, let's
assume that our only hyperparameters are the learning rate and the batch size. We can track their values as well as the
results obtained when using those values like this:

```python
import mlflow

with mlflow.start_run():
    ...
    train and validate...

    # Track values for hyperparameters
mlflow.log_param("learning_rate", learning_rate)
mlflow.log_param("batch_size", batch_size)

# Track results obtained with those values
mlflow.log_metric("val_loss", val_loss)
mlflow.log_metric("val_accuracy", val_accuracy)

# Track artifacts (i.e. files produced by our experiment)
# For example, we can save the weights for the epoch with the
# lowest validation loss
mlflow.log_artifact("best_valid.pt")
```

<br>
<br>

If we do this for all of our experiments, then _mlflow_ will allow us to easily study the results and understand what
works and what doesn't. But you can also look at the results in a notebook by doing:

```python
runs = mlflow.search_runs()
```

We barely scratched the surface about what a tracking tool like mlflow can do for you. For example, they track the code
that runs in your experiment so you can reproduce it even if you changed the code in the meantime. If you are looking to
apply what you are learning in this course in a professional environment, have a good look at tracking tools and how
they can benefit you.

#### Weight Initialization

Weight initialization is a procedure that happens only once, before we start training our neural network. Stochastic
Gradient Descent and other similar algorithms for minimization are iterative in nature. They start from some values for
the parameters that are being optimized and they change those parameters to achieve the minimum in the objective
function (the loss). These "initial values" for the parameters are set through weight initialization.

Before the introduction of BatchNorm, weight initialization was really key to obtaining robust performances. In this
previous era, a good weight initialization strategy could make the difference between an outstanding model and one that
could not train at all. These days networks are much more forgiving. However, a good weight initialization can speed up
your training and also give you a bit of additional performance.

In general, weights are initialized with random numbers close but not equal to zero, not too big but not too small
either. This makes the gradient of the weights in the initial phases of training neither too big nor too small, which
promotes fast training and good performances. Failing to initialize the weights well could result in vanishing or
exploding gradients, and the training would slow down or stop altogether.

Exporting a model for production means packaging your model in a stand-alone format that can be transferred and used to
perform inference in a production environment, such as an API or a website. Remember that the images need some
preprocessing before being fed to the CNN. For example, typically you need to resize, center crop, and normalize the
image with a transform pipeline similar to this:

```python
testval_transforms = T.Compose(
    [
        # The size here depends on your application. Here let's use 256x256
        T.Resize(256),
        # Let's take the central 224x224 part of the image
        T.CenterCrop(224),
        T.ToTensor(),
        T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
    ]
)
```

Obviously, if you do not do these operations in production the performance of your model is going to suffer greatly. The
best course of action is to make these transformations part of your standalone package instead of re-implementing them
in the production environment. Let's see how. We need to wrap our model in a wrapper class that is going to take care of
applying the transformations and then run the transformed image through the CNN. If we trained with the
_nn.CrossEntropyLoss_ as the loss function, we also need to apply a softmax function to the output of the model so that
the output of the wrapper will be probabilities and not merely scores. Let's see an example of such a wrapper class:

```python
import torch
from torchvision import datasets
import torchvision.transforms as T
from __future__ import annotations


class Predictor(nn.Module):

    def __init__(
            self,
            model: nn.Module,
            class_names: list[str],
            mean: torch.Tensor,
            std: torch.Tensor
    ):
        super().__init__()

        self.model = model.eval()
        self.class_names = class_names

        self.transforms = nn.Sequential(
            T.Resize([256, ]),
            T.CenterCrop(224),
            T.ConvertImageDtype(torch.float),
            T.Normalize(mean.tolist(), std.tolist())
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        with torch.no_grad():
            # 1. apply transforms
            x = self.transforms(x)  # =
            # 2. get the logits
            x = self.model(x)  # =
            # 3. apply softmax
            #    HINT: remmeber to apply softmax across dim=1
            x = F.softmax(x, dim=1)  # =

            return x
```

#### The Constructor

Let's first look at the constructor _init:_ we first set the model to eval mode, and we also save the class names. This
will be useful in production: the wrapper will return the probability for each class, so if we take the maximum of that
probability and take the corresponding element from the class_names list, we can return the winning label. Then we have
this:

```python
self.transforms = nn.Sequential(
            T.Resize([256, ]),  # We use single int value inside a list due to torchscript type restrictions
            T.CenterCrop(224),
            T.ConvertImageDtype(torch.float),
            T.Normalize(mean.tolist(), std.tolist())
        )
```

<br>
<br>

This defines the transformations we want to apply. It looks very similar to the transform validation pipeline, with a
few important differences:

1. We do not use _nn.Compose_ but _nn.Sequential_. Indeed the former is not supported by torch.script, the export
   functionality of PyTorch.
2. In Resize the size specification must be a tuple or a list, and not a scalar as we were able to do during training.
3. There is no ToTensor. Instead, we use _T.ConvertImageDtype_. Indeed, in this context the input to the forward method
   is going to be already a Tensor

<br>
<br>

#### Forward Method

Let's now look at the forward method:

```python
def forward(self, x: torch.Tensor) -> torch.Tensor:
    with torch.no_grad():
        # 1. apply transforms
        x = self.transforms(x)  # =
        # 2. get the logits
        x = self.model(x)  # =
        # 3. apply softmax
        #    HINT: remmeber to apply softmax across dim=1
        x = F.softmax(x, dim=1)  # =

        return x
```

We declare we are not going to need gradients with the _torch.no_grad_ context manager. Then, as promised, we first
apply the transforms, then we pass the result through the model, and finally we apply the _softmax_ function to
transform the scores into probabilities. We can now create an instance of our Predictor wrapper and save it to file
using torch script:

<br>

```python
predictor = Predictor(model, class_names, mean, std).cpu()

# Export using torch.jit.script
scripted_predictor = torch.jit.script(predictor)
scripted_predictor.save("standalone_model.pt")
```

Note that we move the Predictor instance to the CPU before exporting it. When reloading the model, the model will be
loaded on the device it was taken from. So if we want to do inference on the CPU, we need to first move the model there.
In many cases CPUs are enough for inference, and they are much cheaper than GPUs. We then use torch.jit.script which
converts our wrapped model into an intermediate format that can be saved to disk ( which we do immediately after). Now,
in a different process or a different computer altogether, we can do:

```python
import torch

predictor_reloaded = torch.jit.load("standalone_model.pt")
```

This will recreate our wrapped model. We can then use it as follows:

```python
from PIL import Image
import torch
import torchvision
import torchvision.transforms as T

# Reload the model
learn_inf = torch.jit.load("standalone_model.pt")

# Read an image and transform it to tensor to simulate what would
# happen in production
img = Image.open("static_images/test/09.Golden_Gate_Bridge/190f3bae17c32c37.jpg")
# We use .unsqueeze because the model expects a batch, so this
# creates a batch of 1 element
pil_to_tensor = T.ToTensor()(img).unsqueeze_(0)

# Perform inference and get the softmax vector
softmax = predictor_reloaded(pil_to_tensor).squeeze()
# Get index of the winning label
max_idx = softmax.argmax()
# Print winning label using the class_names attribute of the
# model wrapper
print(f"Prediction: {learn_inf.class_names[max_idx]}")
```
