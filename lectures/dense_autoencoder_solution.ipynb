{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Autoencoder for Anomaly Detection\n",
    "\n",
    "Anomaly detection is the task of finding anomalous data elements in a dataset. An anomaly is a data element that is an outlier with respect to the rest of the dataset.\n",
    "\n",
    "We are going to train an autoencoder on the MNIST dataset (that only contains numbers), and then we will look into anomalies within the MNIST dataset (i.e., images within MNIST that are somehow different than the rest of the dataset).\n",
    "\n",
    "Even though MNIST is a labeled datasets, we are going to disregard the labels for educational purposes and consider it as an unlabeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to sys.path: /Users/chaklader/Documents/Education/Udacity/Deep_Learning/Projects/2_landmark-classification-cnn\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from helpers import anomaly_detection_display\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure repeatibility\n",
    "np.random.seed(10)\n",
    "\n",
    "\n",
    "project_root_path = str(Path.cwd().parent)\n",
    "\n",
    "if project_root_path not in sys.path:\n",
    "    sys.path.insert(0, project_root_path)\n",
    "    print(f\"Added to sys.path: {project_root_path}\")\n",
    "\n",
    "from src.data import get_data_loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing cached mean and std\n",
      "Dataset mean: tensor([0.4638, 0.4725, 0.4687]), std: tensor([0.2699, 0.2706, 0.3018])\n"
     ]
    }
   ],
   "source": [
    "# This will get data loaders for the MNIST dataset for the train, validation\n",
    "# and test dataset\n",
    "data_loaders = get_data_loaders(batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (3, 224, 224) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(images[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     12\u001b[0m fig, sub \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m)) \n\u001b[0;32m---> 13\u001b[0m sub\u001b[38;5;241m.\u001b[39mimshow(img, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m _ \u001b[38;5;241m=\u001b[39m sub\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ml/lib/python3.12/site-packages/matplotlib/__init__.py:1473\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1473\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\n\u001b[1;32m   1474\u001b[0m             ax,\n\u001b[1;32m   1475\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(sanitize_sequence, args),\n\u001b[1;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: sanitize_sequence(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[1;32m   1478\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1479\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1480\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ml/lib/python3.12/site-packages/matplotlib/axes/_axes.py:5895\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5893\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m-> 5895\u001b[0m im\u001b[38;5;241m.\u001b[39mset_data(X)\n\u001b[1;32m   5896\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5898\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ml/lib/python3.12/site-packages/matplotlib/image.py:729\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    728\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_image_array(A)\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ml/lib/python3.12/site-packages/matplotlib/image.py:697\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    695\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[0;32m--> 697\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (3, 224, 224) for image data"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "# obtain one batch of training images\n",
    "dataiter = iter(data_loaders['train'])\n",
    "images, labels = next(dataiter)\n",
    "images = images.numpy()\n",
    "\n",
    "# get one image from the batch\n",
    "img = np.squeeze(images[0])\n",
    "\n",
    "fig, sub = plt.subplots(figsize = (2,2)) \n",
    "sub.imshow(img, cmap='gray')\n",
    "_ = sub.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### Linear Autoencoder\n",
    "\n",
    "We'll train an autoencoder with these images by flattening them into vectors of length 784. The images from this dataset are already normalized such that the values are between 0 and 1. \n",
    "\n",
    "Here you will build a simple autoencoder. \n",
    "\n",
    "The encoder and decoder should be made of simple Multi-Layer Perceptrons. The units that connect the encoder and decoder will be the _compressed representation_ (also called _embedding_).\n",
    "\n",
    "Since the images are normalized between 0 and 1, you will need to use a **sigmoid activation on the output layer** to get values that match this input value range.\n",
    "\n",
    "For this exercise you are going to use a dimension for the embeddings of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "Dense Autoencoder (dense_autoencoder_solution.ipynb):\n",
    "    A simple autoencoder using fully connected layers.\n",
    "\n",
    "    Architecture:\n",
    "        - Encoder: 28*28 → 128 → 64 → encoding_dim\n",
    "        - Decoder: encoding_dim → 64 → 128 → 28*28\n",
    "        - Activation: ReLU (hidden), Sigmoid (output)\n",
    "        - Input: Flattened 28x28 images (784D)\n",
    "        - Output: Reconstructed 28x28 images (0-1 range)\n",
    "\n",
    "    Use Case: Basic autoencoder for flattened image data\n",
    "    Strengths: Simple, good for understanding autoencoder concepts\n",
    "    Weaknesses: Loses spatial information, less effective for images\n",
    "\n",
    "CNN Autoencoder (cnn_autoencoder.ipynb):\n",
    "    A convolutional autoencoder that preserves spatial information.\n",
    "\n",
    "    Architecture:\n",
    "        - Encoder: \n",
    "            Conv2d(1,16,3) → MaxPool(2) → \n",
    "            Conv2d(16,4,3) → MaxPool(2)  # 4x7x7 bottleneck\n",
    "        - Decoder:\n",
    "            ConvTranspose2d(4,16,2,stride=2) → \n",
    "            ConvTranspose2d(16,1,2,stride=2)\n",
    "        - Activation: ReLU (hidden), Sigmoid (output)\n",
    "        - Input: 1x28x28 images\n",
    "        - Output: Reconstructed 1x28x28 images (0-1 range)\n",
    "\n",
    "    Use Case: Image data where spatial relationships matter\n",
    "    Strengths: Preserves spatial information, better feature extraction\n",
    "    Weaknesses: More complex, requires more parameters\n",
    "\n",
    "Key Differences:\n",
    "    1. Dimensionality:\n",
    "       - Dense: Flattens to 1D (loses spatial info)\n",
    "       - CNN: Maintains 2D structure (preserves spatial info)\n",
    "    \n",
    "    2. Layer Types:\n",
    "       - Dense: Fully connected (Linear) layers\n",
    "       - CNN: Convolutional and transposed convolutional layers\n",
    "    \n",
    "    3. Bottleneck:\n",
    "       - Dense: encoding_dim (configurable)\n",
    "       - CNN: Fixed 4x7x7 feature maps\n",
    "    \n",
    "    4. Performance:\n",
    "       - Dense: Faster training, less memory\n",
    "       - CNN: Better reconstruction quality for images\n",
    "\n",
    "A Convolutional Denoising Autoencoder for image denoising.\n",
    "\n",
    "Architecture:\n",
    "    Encoder (Downsampling Path):\n",
    "        1. Conv2d(1,32,3) + ReLU + BatchNorm + MaxPool(2)\n",
    "            - Input: 1x28x28\n",
    "            - Output: 32x14x14\n",
    "        2. Conv2d(32,16,3) + ReLU + BatchNorm + MaxPool(2)\n",
    "            - Output: 16x7x7\n",
    "        3. Conv2d(16,8,3) + ReLU + BatchNorm + MaxPool(2)\n",
    "            - Output: 8x3x3 (bottleneck)\n",
    "    \n",
    "    Decoder (Upsampling Path):\n",
    "        1. ConvTranspose2d(8,8,3,stride=2) + ReLU + BatchNorm\n",
    "            - Output: 8x7x7\n",
    "        2. ConvTranspose2d(8,16,2,stride=2) + ReLU + BatchNorm\n",
    "            - Output: 16x14x14\n",
    "        3. ConvTranspose2d(16,32,2,stride=2) + ReLU + BatchNorm\n",
    "            - Output: 32x28x28\n",
    "        4. Conv2d(32,1,3) + Sigmoid\n",
    "            - Output: 1x28x28 (reconstructed image)\n",
    "\n",
    "Key Features:\n",
    "    - Uses strided convolutions for downsampling (instead of pooling)\n",
    "    - Batch Normalization after each conv/transposed conv\n",
    "    - ReLU activations for non-linearity\n",
    "    - Sigmoid activation in final layer for pixel values in [0,1]\n",
    "    - Skip connections could be added between encoder/decoder\n",
    "\n",
    "Input/Output:\n",
    "    - Input: Noisy image (1x28x28)\n",
    "    - Output: Denoised image (1x28x28)\n",
    "\n",
    "Note: The final output dimensions might need adjustment based on \n",
    "input size due to the transposed convolution operations.\n",
    "\"\"\"\n",
    "# define the NN architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        ## encoder ##\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, encoding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(encoding_dim),\n",
    "\n",
    "        )\n",
    "        \n",
    "        ## decoder ##\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 28*28),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.auto_encoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            self.encoder,\n",
    "            self.decoder\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define feedforward behavior \n",
    "        # and scale the *output* layer with a sigmoid activation function\n",
    "        \n",
    "        encoded = self.auto_encoder(x)\n",
    "        \n",
    "        # Reshape the output as an image\n",
    "        # remember that the shape should be (batch_size, channel_count, height, width)\n",
    "        return encoded.reshape((x.shape[0], 1, 28, 28))\n",
    "    \n",
    "# initialize the NN\n",
    "encoding_dim = 32\n",
    "model = Autoencoder(encoding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Loss Function\n",
    "\n",
    "As explained in the lesson, we can use the Mean Squared Error loss, which is called `MSELoss` in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training loop is similar to a normal training loop, however, this task is an unsupervised task. That means we do not need labels. The MNIST dataset does provide labels, of course, so we will just disregard them.\n",
    "\n",
    "For this simple autoencoder we do not need the GPU, so we will train on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "        \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data in tqdm(desc=\"Training\", total=len(data_loaders['train']), iterable=data_loaders['train']):\n",
    "        # we disregard the labels. We use the Python convention of calling\n",
    "        # an unused variable \"_\"\n",
    "        images, _ = data\n",
    "\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(images)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs.flatten(), images.flatten())\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(desc=\"Validating\", total=len(data_loaders['valid']), iterable=data_loaders['valid']):\n",
    "            # _ stands in for labels, here\n",
    "            images, _ = data\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            outputs = model(images)\n",
    "            # calculate the loss\n",
    "            loss = criterion(outputs.flatten(), images.flatten())\n",
    "            \n",
    "            # update running training loss\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    # print avg training statistics\n",
    "    train_loss /= len(data_loaders['train'])\n",
    "    val_loss /= len(data_loaders['valid'])\n",
    "    print(\"Epoch: {} \\tTraining Loss: {:.6f}\\tValid Loss: {:.6f}\".format(epoch, train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Anomalies\n",
    "Now that our autoencoder is trained we can use it to find anomalies. Let's consider the test set. We loop over all the batches in the test set and we record the value of the loss for each example separately. The examples with the highest reconstruction loss are our anomalies. \n",
    "\n",
    "Indeed, if the reconstruction loss is high, that means that our trained autoencoder could not reconstruct them well. Indeed, what the autoencoder learned about our dataset during training is not enough to describe these examples, which means they are different than what the encoder has seen during training, i.e., they are anomalies (or at least they are the most uncharacteristic examples).\n",
    "\n",
    "Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this dataset is small we collect all the losses as well as\n",
    "# the image and its reconstruction in a dictionary. In case of a\n",
    "# larger dataset you might have to save on disk\n",
    "# (won't fit in memory)\n",
    "losses = {}\n",
    "\n",
    "# We need the loss by example (not by batch)\n",
    "loss_no_reduction = nn.MSELoss(reduction='none')\n",
    "\n",
    "idx = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(desc=\"Testing\", total=len(data_loaders['test']),\n",
    "            iterable=data_loaders['test']\n",
    "        ):\n",
    "\n",
    "            images, _ = data\n",
    "                        \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # calculate the loss\n",
    "            loss = loss_no_reduction(outputs, images)\n",
    "            \n",
    "            # Accumulate results per-example\n",
    "            for i, l in enumerate(loss.mean(dim=[1, 2, 3])):\n",
    "                losses[idx + i] = {\n",
    "                    'loss': float(l.cpu().numpy()),\n",
    "                    'image': images[i].numpy(),\n",
    "                    'reconstructed': outputs[i].numpy()\n",
    "                }\n",
    "            \n",
    "            idx += loss.shape[0]\n",
    "\n",
    "# Let's save our results in a pandas DataFrame\n",
    "df = pd.DataFrame(losses).T\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now display the histogram of the loss. The elements on the right (with the higher loss) are the most uncharacteristic examples. Feel free to look into `helpers.py` to see how these plots are made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import anomaly_detection_display\n",
    "\n",
    "anomaly_detection_display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the bottom panels has the input in the first row and the reconstruction in the second row. \n",
    "\n",
    "Let's look at the first of the two panels. The most difficult numbers to reconstruct (the \"anomalies\" with the highest loss) are indeed pretty particular: they have some noise (like the vertical lines in some of the numbers), or are just not standard ways of drawing the respective numbers. As a result, the reconstructed images (second row) are not matching the inputs very well and the loss is high. These are anomalies.\n",
    "\n",
    "The second panel instead shows numbers taken from the peak of the distribution, and look indeed much more standard. The autoencoder can reconstruct them much better which result in a lower loss.\n",
    "\n",
    "In summary, the reconstruction loss can be used as a score proportional to how much a certain example is typical: a low loss means a typical example; a high loss means an atypical example,an anomaly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
