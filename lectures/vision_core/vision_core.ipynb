{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4e165a8",
   "metadata": {},
   "source": [
    "This codebase can be used for many purposes including:\n",
    "\n",
    "1. **Object Detection Pipeline**:\n",
    "- Train/evaluate models (Faster R-CNN, Mask R-CNN, etc)\n",
    "- Process COCO-format datasets\n",
    "- Calculate evaluation metrics (mAP, IoU)\n",
    "\n",
    "2. **Key Components Available:**\n",
    "\n",
    "Data Preparation:\n",
    "- Image transforms/normalization\n",
    "- Annotation processing\n",
    "- Dataset utilities\n",
    "\n",
    "Training Infrastructure:\n",
    "- Distributed training setup\n",
    "- Learning rate scheduling\n",
    "- Loss tracking\n",
    "\n",
    "Evaluation Tools:\n",
    "- COCO metric calculation\n",
    "- Prediction visualization\n",
    "- Model performance analysis\n",
    "\n",
    "3. **Experiment Ideas:**\n",
    "- Transfer Learning:\n",
    "- Fine-tune pretrained models on custom data\n",
    "- Modify head architectures\n",
    "- Data Augmentation:\n",
    "- Test different transform combinations\n",
    "- Visualize augmented samples\n",
    "\n",
    "4. **Performance Optimization:**\n",
    "- Benchmark training speed\n",
    "- Profile memory usage\n",
    "- Test mixed-precision\n",
    "\n",
    "5. **Visualization Capabilities:**\n",
    "- Plot training metrics\n",
    "- Visualize predictions vs ground truth\n",
    "- Generate confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494646b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Starting training for 3 epochs...\n",
      "Epoch: [0]  [ 0/50]  eta: 0:02:59  lr: 0.000107  loss: 4.8147 (4.8147)  loss_classifier: 0.0143 (0.0143)  loss_box_reg: 0.0001 (0.0001)  loss_objectness: 0.2087 (0.2087)  loss_rpn_box_reg: 4.5915 (4.5915)  time: 3.5884  data: 0.0033\n",
      "Epoch: [0]  [10/50]  eta: 0:02:09  lr: 0.001126  loss: 13.2961 (22.3868)  loss_classifier: 0.0214 (0.0233)  loss_box_reg: 0.0001 (0.0002)  loss_objectness: 0.2087 (0.2198)  loss_rpn_box_reg: 12.9991 (22.1435)  time: 3.2410  data: 0.0022\n"
     ]
    }
   ],
   "source": [
    "# In vision_core.ipynb\n",
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# ------------------- PATH SETUP -------------------\n",
    "# This assumes your notebook is in 'lectures/vision_core/'\n",
    "# and 'vision_core' is the package directory itself.\n",
    "# Adds the 'lectures' directory to sys.path so 'from vision_core import ...' works.\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\" if \"__file__\" in locals() else os.getcwd()))\n",
    "parent_dir_of_vision_core = os.path.dirname(notebook_dir) # This should be 'lectures'\n",
    "if parent_dir_of_vision_core not in sys.path:\n",
    "    sys.path.append(parent_dir_of_vision_core)\n",
    "\n",
    "# Now import from vision_core\n",
    "from vision_core import utils, engine # Assuming transforms is not strictly needed for this example\n",
    "\n",
    "# ------------------- DEVICE SETUP -------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ------------------- CUSTOM COLLATE FUNCTION FOR DETECTION -------------------\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# ------------------- MOCK DATASET -------------------\n",
    "class FakeDataset(Dataset):\n",
    "    def __init__(self, num_samples=100, image_size=(3, 300, 400)):\n",
    "        self.num_samples = num_samples\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Create a random image\n",
    "        image = torch.rand(self.image_size)\n",
    "\n",
    "        # Create a corresponding target dictionary\n",
    "        # Faster R-CNN expects specific keys with specific data types\n",
    "        num_boxes = torch.randint(1, 5, (1,)).item() # Random number of boxes\n",
    "        \n",
    "        boxes = torch.rand(num_boxes, 4) * torch.tensor([self.image_size[2], self.image_size[1], self.image_size[2], self.image_size[1]])\n",
    "        # Ensure x2 > x1 and y2 > y1\n",
    "        boxes[:, 2] = torch.clamp(boxes[:, 0] + boxes[:, 2]* (self.image_size[2] - boxes[:,0]), min=boxes[:,0]+1)\n",
    "        boxes[:, 3] = torch.clamp(boxes[:, 1] + boxes[:, 3]* (self.image_size[1] - boxes[:,1]), min=boxes[:,1]+1)\n",
    "        \n",
    "        labels = torch.randint(1, 91, (num_boxes,), dtype=torch.int64) # COCO has 90 classes + background\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes.to(dtype=torch.float32)\n",
    "        target[\"labels\"] = labels.to(dtype=torch.int64)\n",
    "        target[\"image_id\"] = torch.tensor([idx], dtype=torch.int64)\n",
    "        target[\"area\"] = (target[\"boxes\"][:, 3] - target[\"boxes\"][:, 1]) * \\\n",
    "                         (target[\"boxes\"][:, 2] - target[\"boxes\"][:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros((num_boxes,), dtype=torch.uint8) # uint8 or int64 depending on PyTorch version, uint8 is safer for older versions\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    # This method is needed for engine.evaluate if using COCO-style evaluation\n",
    "    def get_coco_api(self):\n",
    "        # Return a mock COCO API or a real one if you have annotations\n",
    "        from pycocotools.coco import COCO\n",
    "        mock_coco = COCO()\n",
    "        # You might need to populate it with dataset_ids if evaluate uses it\n",
    "        # For this example, a minimal mock should suffice to avoid AttributeError\n",
    "        mock_coco.dataset = {'images': [{'id': i} for i in range(self.num_samples)], 'annotations': []}\n",
    "        mock_coco.createIndex()\n",
    "        return mock_coco\n",
    "\n",
    "# ------------------- MODEL AND OPTIMIZER -------------------\n",
    "# Initialize model\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True, num_classes=91) # Ensure num_classes matches your dataset (COCO default is 91)\n",
    "model.to(device)\n",
    "\n",
    "# Initialize optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Learning rate scheduler (optional, but good practice)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# ------------------- DATA LOADERS -------------------\n",
    "train_dataset = FakeDataset(num_samples=100)\n",
    "val_dataset = FakeDataset(num_samples=50) # Separate validation set\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=2, \n",
    "    shuffle=True, \n",
    "    num_workers=0, # Set to 0 for simplicity, can increase if not on Windows or if __main__ guard is used\n",
    "    collate_fn=collate_fn # Use the custom collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=1, # Typically 1 for validation in detection\n",
    "    shuffle=False, \n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# ------------------- TRAINING AND VALIDATION LOOP -------------------\n",
    "num_epochs = 3\n",
    "print(f\"Starting training for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train for one epoch\n",
    "    engine.train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=10)\n",
    "    \n",
    "    # Update the learning rate\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    # The evaluate function in torchvision's engine.py expects a COCO-like API from the dataset\n",
    "    engine.evaluate(model, val_loader, device=device)\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b8e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of making a prediction (after training)\n",
    "model.eval()\n",
    "sample_image, _ = val_dataset[0]\n",
    "with torch.no_grad():\n",
    "    prediction = model([sample_image.to(device)])\n",
    "print(\"\\nExample prediction:\")\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aipnd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
