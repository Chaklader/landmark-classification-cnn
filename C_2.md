# Convolutional Neural Networks

<br>
<br>

# C-2: CNN Concepts

<br>
<br>

A CNN is a neural network developed specifically for image and video processing. CNNs are used to automate many tasks,
for example:

- Image classification
- Object recognition
- Anomaly detection
- Image captioning

They are widely used across many industries, including medicine, banking, manufacturing, insurance, real-estate,
transportation (self-driving cars), and social networks. CNNs are arguably the main technology responsible for the
Artificial Intelligence renaissance we are witnessing today, with a billion of dollars in revenue generated by their
application.

<p align="center">
<img src="images/cnn_o.png" alt="CNN Overview" width="600" height=auto>
</p>
<p align="center">figure: Overview of a Convolutional Neural Network applications</p>

<br>
<br>

CNNs are powerful for image tasks, overcoming MLP limitations. The course covers CNN basics, advanced architectures, and
applications like object detection and segmentation. If you use CNNs to power a real product in a real-world setting,
you are going to interact with several different profiles:

- Data Scientist / Machine Learning Engineer: Responsible for developing the ML pipeline and the model, as well as
  performing all the relevant analytics - for example on data quality and performance measurements.
- Data Engineers: Responsible for the data ingestion pipelines, the quality of the data that the DS/MLE receive, and for
  provisioning the right data at inference time.
- Software Engineers: Responsible for the production environment, both front-end and back-end, as well as for many
  pieces of the MLOps infrastructure. Involve them from the beginning to make sure that the model you are producing can
  fit into the production environment.
- DevOps Engineers: Help in handling the infrastructure, including training servers, various MLOps tools, and other
  infrastructure needed to train and deploy a model.
- Product Managers: Define the right problem to solve, exploit the knowledge of the customers, and define quantifiable
  deliverables and success criteria for the project. The PM also helps in keeping the project on time and on budget.
- Customers: Consumer of the product; we should always consider the customers' and users' perspectives for every
  decision that we make.

<p align="center">
<img src="images/pre.png" alt="Customer" width="600" height=auto>
</p>
<p align="center">figure: Pre-process the data</p>

An image is seen by the computer as an array of values, namely, a matrix. The images in the MNIST dataset are 28 x 28
and 8-bit grayscale. This means that the computer represents each one of them as a square matrix of 28 x 28 elements,
where the value in the element of the matrix represents the light intensity with a range of 0 to 255: 0 means pure black
and 255 means pure white.

We already know how to perform classification with neural networks, and in particular with a Multi-Layer Perceptron,
MLP. This network takes as input a grayscale image as a matrix and outputs a vector of scores or a vector of
probabilities ( one for each class). The class corresponding to the maximum of that vector corresponds to the best guess
for the label of the image.

#### Multi-Layer Perceptrons (MLPs)

A Multi-Layer Perceptron (MLP) is a type of feedforward artificial neural network consisting of at least three layers:

- An **input layer** that receives the data
- One or more **hidden layers** that perform computations
- An **output layer** that produces the final result

#### How MLPs Work

##### Input Processing

- For image data, the 2D matrix is flattened into a 1D vector
- Each input neuron represents a single feature (e.g., a pixel value)

##### Forward Propagation

- Data flows from input to output in one direction
- Each neuron applies a weighted sum of inputs plus a bias
- The result is passed through an activation function

##### Activation Functions

- ReLU (Rectified Linear Unit): Outputs the positive part of its input
- Sigmoid: Maps input to a value between 0 and 1
- Tanh: Maps input to a value between -1 and 1

#### Architecture Design

- **Input layer size**: Fixed by the data dimension (e.g., 784 for MNIST)
- **Output layer size**: Fixed by the task (e.g., 10 for digit classification)
- **Hidden layer size**: Often start with the mean between input and output dimensions
- **Number of hidden layers**: Start with 1-2 and increase if needed

#### Training Process

1. **Initialize weights**: Small random values
2. **Forward pass**: Compute predictions
3. **Calculate loss**: Typically cross-entropy for classification
4. **Backward pass**: Compute gradients
5. **Update weights**: Using an optimizer like SGD or Adam

#### Limitations for Image Data

- **Loss of spatial information**: Flattening destroys the 2D structure
- **No translation invariance**: Objects must be seen in all possible positions
- **Parameter inefficiency**: Fully connected layers have many parameters
- **Difficulty with complex patterns**: Struggles to learn hierarchical features

#### When to Use MLPs

- Tabular data
- Simple pattern recognition
- Data without spatial or temporal relationships
- When input features are independent

MLPs provide the foundation for understanding more complex neural networks like CNNs, which address many of these
limitations for image processing tasks.

##### Flattening

Suppose we want to use an MLP to classify our image. The problem is, the network takes a 1d array as input, while we
have images that are 28x28 matrices. The obvious solution is to flatten the matrix, i.e., to stack all the rows of the
matrix in one long 1D vector, as in the image below.

<p align="center">
<img src="images/flat.png" alt="Flattening Process" width="600" height=auto>
</p>
<p align="center">figure: flattening of the matrix</p>

<br>
<br>

##### Normalizing Image Inputs

##### Data Normalization

Data normalization is a critical preprocessing step in neural networks that transforms input data to have similar
scales. For image data, this means converting pixel values (typically 0-255) to smaller ranges.

##### Why Normalize Data?

1. **Faster convergence**: When inputs are normalized (close to zero), gradient descent works more efficiently
2. **Improved stability**: Prevents extremely large or small weight updates during training
3. **Better learning**: Helps the network learn the relative importance of features rather than being affected by their
   scales

##### Normalization Methods for Images:

1. **Simple scaling**: Divide pixel values by 255 to get values between 0 and 1
2. **Standardization**: Subtract mean and divide by standard deviation to get:
   $$x_{normalized} = \frac{x - \mu}{\sigma}$$
3. **Approximation method**: Use mean=0.5 and standard deviation=0.5 to center values:
   $$x_{normalized} = \frac{x/255 - 0.5}{0.5}$$

##### Activation Functions

Activation functions introduce non-linearity into neural networks, allowing them to learn complex patterns.

###### Sigmoid Activation Function

- **Mathematical form**: $$\sigma(z) = \frac{1}{1 + e^{-z}}$$

- **Output range**: Between 0 and 1

- Characteristics:

    - S-shaped curve
    - Highest rate of change around z=0
    - Almost flat (saturated) when z is very positive or very negative
    - Derivative: $$\sigma'(z) = \sigma(z)(1 - \sigma(z))$$
    - Maximum derivative of 0.25 at z=0

###### ReLU (Rectified Linear Unit) Activation Function

The purpose of an activation function is to scale the outputs of a layer so that they are consistent, small values. Much
like normalizing input values, this step ensures that our model trains efficiently!

A ReLU activation function stands for _Rectified Linear Unit_ and is one of the most commonly used activation functions
for hidden layers. It is an activation function, simply defined as the positive part of the input, x. So, for an input
image with any negative pixel values, this would turn all those values to 0, black. You may hear this referred to as
_clipping_ the values to zero; meaning that is the lower bound.

<p align="center">
<img src="images/ReLU.png" alt="ReLU Activation Function" width="600" height=auto>
</p>
<p align="center">figure:  ReLU Activation Function</p>

- **Mathematical form**: $$R(z) = \max(0, z)$$
- **Output range**: From 0 to ∞
- Characteristics:

    - Simple piecewise linear function
    - Zero output for negative inputs
    - Linear for positive inputs
    - Derivative: $$R'(z) = \begin{cases} 0 & \text{if } z < 0 \ 1 & \text{if } z > 0 \end{cases}$$
    - Undefined at z=0, but in practice set to 0 or 1

##### Connection Between Normalization and Activation Functions

When we normalize data to be close to zero and initialize weights near zero:

1. The product of weights and inputs results in values near zero: $$z = w \cdot x + b \approx 0$$
2. At z≈0, activation functions have their largest derivatives:
    - For sigmoid: maximum derivative of 0.25
    - For ReLU: abrupt change from 0 to 1
3. This maximizes the effect of gradient descent:
   $$\Delta w = -\eta \frac{\partial L}{\partial w} = -\eta \frac{\partial L}{\partial a} \cdot \frac{\partial a}{\partial z} \cdot \frac{\partial z}{\partial w}$$

Where the middle term $$\frac{\partial a}{\partial z}$$ is the derivative of the activation function, which is largest
near zero.

This is why normalizing inputs to be close to zero allows for faster training and better convergence in neural networks.
Data normalization is an important pre-processing step for neural networks. The activation functions that are normally
used in neural networks (sigmoid, ReLU, ...) have the highest rate of change around 0:

<p align="center">
<img src="images/act.png" alt="Activation Functions" width="600" height=auto>
</p>
<p align="center">figure:  Normalizing Image Inputs</p>

<br>
<br>

This means that their derivative is large for inputs that are not too far from 0. Since we train neural networks with
gradient descent, the training can proceed faster if the weights stay in the vicinity of where the activation function
changes rapidly, i.e., close to 0.

The weights of a neural network are typically initialized with a mean of 0, i.e., some weights are negative and some are
positive, but they are in general between -1 and +1, close to 0. Remember that these weights are multiplied with the
feature values (in this case the pixel values) and then a bias is added on top before the result is fed to the
activation function.

Therefore, if we want the input of the activation function to be somewhere close to 0, we need to start with a number
that is close to zero, because two numbers close to zero multiplied together give another number close to zero. So we
need to take the pixels in the input image, which in the case of a grayscale image have values between 0 and 255, and
renormalize them to be close to zero. The easiest way is to just divide the value by 255, thereby changing the pixel
values to be between 0 and 1. In many cases, we go further than that: we compute the mean and the standard deviation of
all the pixels in the renormalized dataset, then we subtract the mean and divide by the standard deviation for each
image separately. Therefore, our transformed data will contain both negative and positive pixels with mean 0 and
standard deviation 1. Sometimes you'll see an approximation here, where we use a mean and standard deviation of 0.5 to
center the pixel values.

#### Dropout

Dropout is a regularization technique that helps prevent neural networks from overfitting. Here's how it functions:

1. **During training**:
    - Randomly deactivates (or "drops out") a percentage of neurons in a layer
    - If the `Dropout(0.5)`, it means 50% of neurons are randomly disabled each batch
    - Each neuron has a 50% probability of being temporarily removed
2. **Neuron deactivation**:
    - Set the output of dropped neurons to zero
    - The remaining neurons must compensate for the missing ones
    - This forces the network to be more robust and redundant
3. **Scaling mechanism**:
    - Remaining active neurons are scaled up by 1/(1-p) where p is dropout rate
    - With 50% dropout, active neurons' outputs are doubled
    - This keeps the expected sum of activations the same
4. **During inference (testing)**:
    - No neurons are dropped out
    - Instead, all neurons are active but their outputs are scaled down
    - Alternatively, during training the active neurons are scaled up
5. **Benefits**:
    - Prevents co-adaptation (neurons becoming too interdependent)
    - Acts like an ensemble of different networks
    - Reduces overfitting by preventing memorization

Dropout essentially makes the network learn multiple redundant ways to classify each sample, making it more robust and
resistant to overfitting.

<br>
<br>

#### MLP for MNIST Dataset

The input of our MLP must obviously be 28 x 28=784, which corresponds to the dimension of the flattened image. The
output of the MLP must also be obviously a vector with 10 elements (e.g. 0 to 9). The values in this vector are
proportional to the probability that the network assigns to each class. So if the network thinks that it is most likely
that the given image is an 8, then the element in the array corresponding with 8 should be the largest. But what goes
between the input and the output, how many hidden layers, and how many neurons.

<p align="center">
<img src="images/tra.png" alt="MLP Architecture" width="600" height=auto>
</p>
<p align="center">figure:  A Multi-Layer Perceptron for MNIST</p>

<br>
<br>

<p align="center">
<img src="images/tra_1.png" alt="Training Process" width="600" height=auto>
</p>
<p align="center">figure: Visualization of the neural network training process and data flow</p>

<br>
<br>

#### Loss Function

The loss function quantifies how far we are from the ideal state where the network does not make any mistakes and has
perfect confidence in its answers. Depending on the task and other considerations we might pick different loss
functions. For image classification the most typical loss function is the _Categorical Cross-Entropy (CCE)_ loss,
defined as:

$$
\text{CCE} = -\sum_{i=1}^{n_\text{classes}} y_i \log(\hat{p}_i)
$$

Where:

- $n_\text{classes}$ is the number of classes (10 for MNIST digits)
- $y_i$ is the true label (ground truth) as a one-hot encoded vector of length 10
- $\hat{p}_i$ is the predicted probability for class $i$

<br>
<br>

The interpretations of the Loss Function are:

1. The loss quantifies the difference between the predicted probability distribution and the true distribution (one-hot
   encoded ground truth).
2. A perfect prediction would result in a loss of 0, while incorrect predictions increase the loss value.
3. The logarithm heavily penalizes confident misclassifications, encouraging the model to be cautious with its
   predictions.

This loss function is well-suited for our MNIST task because:

1. It naturally handles multi-class problems.
2. It encourages the model to output well-calibrated probabilities.
3. It's differentiable, allowing for effective backpropagation during training.

By minimizing this loss during training, we push our model to make increasingly accurate predictions on the digit
classification task.

<br>
<br>

#### Design of an MLP

When designing an MLP you have a lot of different possibilities, and it is sometimes hard to know where to start.
Unfortunately there are no strict rules, and experimentation is key. However, here are some guidelines to help you get
started with an initial architecture that makes sense, from which you can start experimenting.

The number of inputs _input_dim_ is fixed (in the case of MNIST images for example it is 28 x 28 = 784), so the first
layer must be a fully-connected layer (Linear in PyTorch) with _input_dim_ as input dimension.

Also, the number of outputs is fixed as it is determined by the desired outputs. For a classification problem it is the
number of classes, _n_classes_, and for a regression problem it is 1 (or the number of continuous values to predict). So
the output layer is a Linear layer with _n_classes_ (in case of classification).

What remains to be decided is the number of _hidden layers_ and their _size_. Typically, you want to start from only one
hidden layer, with a number of neurons between the input and the output dimension. Sometimes adding a second hidden
layer helps, and in rare cases you might need to add more than one. But one is a good starting point.

As for the number of neurons in the hidden layers, a decent starting point is usually the mean between the input and the
output dimension. Then you can start experimenting with increasing or decreasing, and observe the performances you get.
If you see overfitting, start by adding regularization (dropout and weight decay) instead of decreasing the number of
neurons, and see if that fixes it. A larger network with a bit of drop-out learns multiple ways to arrive to the right
answer, so it is more robust than a smaller network without dropout. If this doesn't address the overfitting, then
decrease the number of neurons. If you see underfitting, add more neurons. You can start by approximating up to the
closest power of 2. Keep in mind that the number of neurons also depends on the size of your training dataset: a larger
network is more powerful, but it needs more data to avoid overfitting. The Key Points for the MLP Design are:

1. **Input layer size**: Fixed by input data dimensions
2. **Output layer size**: Fixed by classification or regression task requirements
3. **Hidden layers**: Start with one hidden layer as baseline
4. **Hidden layer sizing**: Begin with mean between input and output dimensions
5. **Regularization first**: Add dropout before reducing model size if overfitting
6. **Scale with data**: Larger networks need more training data
7. **Adjust incrementally**: Experiment by gradually changing architecture
8. **Monitor performance**: Use validation data to guide architecture decisions

So let's consider the MNIST classification problem. We have _n_classes_ = 10 and _input_dim_ = 784, so a starting point
for our experimentation could be:

<br>
<br>

```python
import torch
import torch.nn as nn


class MLPModel(nn.Module):

    def __init__(self):
        super().__init__()

        # Create layers. In this case just a standard MLP
        self.model = nn.Sequential(
            # Input layer. The input is obviously 784. For
            # the output (which is the input to the hidden layer)
            # we take the mean between network input and output:
            # (784 + 10) / 2 = 397 which we round to 400
            nn.Linear(784, 400),
            nn.Dropout(0.5),  # Combat overfitting
            nn.ReLU(),
            # Hidden layer
            nn.Linear(400, 400),
            nn.Dropout(0.5),  # Combat overfitting
            nn.ReLU(),
            # Output layer, must receive the output of the
            # hidden layer and return the number of classes
            nn.Linear(400, 10)
        )

    def forward(self, x):
        # nn.Sequential will call the layers
        # in the order they have been inserted
        return self.model(x)
```

This code defines an MLP model for MNIST digit classification. It contains two hidden layers, both with 400 neurons. The
network takes flattened 28×28 pixel images (784 inputs), processes them through the first hidden layer with ReLU
activation and 50% dropout, then through an identical second hidden layer, and finally outputs 10 values corresponding
to digit class scores. The model uses a sequential architecture where data flows linearly through each layer. The
dropout layers help prevent overfitting by randomly deactivating 50% of neurons during training. The forward method
simply passes the input through the sequential model, returning the final output without applying any additional
activation function (allowing it to be used with CrossEntropyLoss which includes softmax internally).

<br>
<br>

#### Sequential vs. Non-Sequential Neural Networks

Sequential and Non-Sequential networks represent different ways to structure neural networks:

##### Sequential Neural Networks

1. **Definition**: Linear stack of layers where output of one layer becomes input to the next
2. **Data flow**: Strictly forward in one direction
3. **Structure**: Simple chain-like arrangement of layers
4. **Implementation**: Often implemented using `nn.Sequential` in PyTorch
5. **Best for**: Straightforward tasks with simple data flow requirements

A sequential neural network is a linear stack of layers where the output of one layer becomes the input to the next
layer. This is the simplest and most common type of neural network architecture. The main characteristics of a
sequential neural network are:

1. Layers are arranged in a straight line.
2. Data flows through the network in a single, forward direction.
3. Easy to conceptualize and implement.

##### Non-Sequential Neural Networks

1. **Definition**: Networks with complex connectivity patterns beyond simple chains
2. **Data flow**: Can branch, merge, skip connections, or have feedback loops
3. **Structure**: Can have multiple inputs/outputs or complex internal connections
4. **Implementation**: Typically requires custom `forward()` method definition
5. Types include:
    - Networks with skip connections (ResNets)
    - Multi-path networks
    - Networks with shared components
    - Graph-based networks

It is absolutely possible, and often necessary, to have neural networks that are not sequential. They are sometimes also
called non-linear neural networks. The types of Non-Sequential Architectures are provided below:

1. Branching Networks: Where the data flow splits and merges.
2. Residual Networks (ResNets): Include skip connections that bypass one or more layers.
3. Recurrent Neural Networks (RNNs): Have feedback loops, allowing information to persist.
4. Graph Neural Networks: Operate on graph-structured data.
5. Multi-Input or Multi-Output Networks: Accept or produce multiple tensors.

##### When to Use Non-Sequential Networks

1. Complex Data Dependencies: When your data has complex relationships that can't be captured by a simple linear flow.
2. Performance Improvement: Techniques like skip connections in ResNets can help with training very deep networks.
3. Task-Specific Requirements: Some tasks inherently require non-sequential processing, like image segmentation or
   machine translation.
4. Multi-Modal Data: When working with multiple types of input data simultaneously.

Non-sequential architectures offer more flexibility and can often capture more complex patterns in data, but they can
also be more challenging to design and train. The key differences of them with the sequential architectures are:

1. **Flexibility**: Non-Sequential offers more design flexibility
2. **Complexity**: Sequential is simpler to implement and understand
3. **Use cases**: Complex tasks often require non-sequential architectures
4. **Implementation complexity**: Sequential can be defined with one container, while non-sequential requires more code

The architecture choice depends on the specific requirements of your task - Sequential is sufficient for many problems,
while complex tasks may benefit from non-sequential designs.

<br>

#### Training Neural Networks

Imagine you're teaching a child to recognize different animals. At first, they make many mistakes, but with each example
you show and correct, they gradually improve. Training a neural network follows a similar pattern. The process begins
with preparing your data, designing a model architecture, and then guiding the model through thousands of examples where
it makes predictions, receives feedback on its errors, and adjusts its understanding accordingly.

This iterative learning cycle is powered by a mathematical process called gradient descent, where the model
systematically adjusts its internal parameters (weights and biases) to minimize the difference between its predictions
and the actual answers. Think of it as the model slowly finding its way down a mountain of errors to reach the valley of
good predictions. Let's explore each component of this fascinating process in detail. The essential components of Neural
Network training are following:

##### Loss Function

The loss function serves as our measure of how wrong the model's predictions are. For classification tasks like
identifying digits or categorizing images, we typically use Cross Entropy Loss. This function has an elegant
mathematical property: it severely penalizes the model when it's confidently wrong, but only mildly when it's slightly
off.

Cross Entropy Loss combines two operations: first, it converts raw model outputs into probabilities using the softmax
function, and then it measures how different these probabilities are from the true labels. In PyTorch, this is
conveniently packaged in `nn.CrossEntropyLoss()`. The lower this number, the better our model is performing.

##### Optimizer

While the loss function tells us how far off we are, the optimizer determines how to adjust the model parameters to
improve. Think of the optimizer as a navigator helping the model traverse the complex landscape of possible parameter
values. Popular optimizers include:

**Stochastic Gradient Descent (SGD)**: The classic approach that moves parameters in the direction that reduces the
loss, scaled by a learning rate. Like taking careful steps down our error mountain.

**Adam**: A more sophisticated navigator that adapts its step size based on recent history, often reaching good
solutions faster. Adam maintains separate learning rates for each parameter and adjusts them based on recent gradients.

When configuring an optimizer, you'll specify which parameters to update (typically all model parameters with
`model.parameters()`), the learning rate (how big each adjustment step should be), and sometimes weight decay (a
technique to prevent overfitting by keeping parameters small).

##### Training Loop

The training loop is where the actual learning happens. For a specified number of epochs (complete passes through the
dataset), we repeatedly:

1. Put the model in training mode with `model.train()` to enable dropout and other training-specific behaviors.
2. For each batch of data:
    - Clear previous gradients with `optimizer.zero_grad()` to start fresh.
    - Perform a forward pass by feeding data through the model: `output = model(data)`.
    - Calculate the loss between the model's output and the true targets.
    - Compute gradients with `loss.backward()`, which determines how each parameter should change.
    - Update parameters with `optimizer.step()`, which applies these changes according to the optimizer's rules.
3. Track the average loss to monitor progress.

This cycle of prediction, error measurement, and adjustment is the essence of how neural networks learn.

##### Validation

After each training epoch, we need to assess how well our model generalizes to data it hasn't seen before. The
validation process:

1. Disables gradient calculation with `with torch.no_grad():` since we're not learning here, just evaluating.

2. Sets the model to evaluation mode with `model.eval()`, which affects layers like Dropout.

3. Runs predictions on the validation data and calculates metrics like loss and accuracy.

4. Compares validation performance to previous epochs.

Validation serves two crucial purposes:

- It tells us how well our model will likely perform on new data.
- It helps detect overfitting, where training performance improves but validation performance worsens, indicating the
  model is memorizing training data rather than learning generalizable patterns.

A useful practice is saving model weights whenever validation performance reaches a new high, ensuring we keep our best
model even if performance later deteriorates. Once we have performed an epoch of training we can evaluate the model
against the validation set to see how it is doing. This is accomplished with the validation loop:

```python
# Tell pytorch to stop computing gradients for the moment
# by using the torch.no_grad() context manager
with torch.no_grad():
    # set the model to evaluation mode
    # This changes the behavior of some layers like
    # Dropout with respect to their behavior during
    # training
    model.eval()

    # Keep track of the validation loss
    valid_loss = 0.0

    # Loop over the batches of validation data
    # (here we have removed the progress bar display that is
    # accomplished using tqdm in the video, for clarity)
    for batch_idx, (data, target) in enumerate(valid_dataloader):
        # 1. forward pass: compute predicted outputs by passing inputs to the model
        output = model(data)

        # 2. calculate the loss
        loss_value = criterion(output, target)

        # Calculate average validation loss
        valid_loss = valid_loss + (
                (1 / (batch_idx + 1)) * (loss_value.data.item() - valid_loss)
        )

    # Print the losses
    print(f"Epoch {epoch + 1}: training loss {train_loss:.5f}, valid loss {valid_loss:.5f}")

```

It is usually a good idea to wrap the validation loop in a function so you can return the validation loss for each
epoch, and you can check whether the current epoch has the lowest loss so far. In that case, you save the weights of the
model. We will see in one of the future exercises how to do that.

##### Testing Phase

Once training is complete, we evaluate the model on a completely unseen test dataset. This process is nearly identical
to validation but serves as the final, unbiased assessment of our model's capabilities.

##### Monitoring and Improving

Visualizing both training and validation losses over time creates learning curves that reveal important patterns:

- If both training and validation losses remain high, the model is underfitting and might need more capacity or training
  time.
- If training loss is much lower than validation loss, the model is overfitting and might need regularization techniques
  like dropout or weight decay.
- The optimal stopping point occurs when validation loss reaches its minimum before rising again.

<p align="center">
<img src="images/train.png" alt="Training and Validation Loss" width="600" height=auto>
</p>
<p align="center">figure: Validation of a Convolutional Neural Network </p>

##### Training Workflow

The entire process forms a cycle:

1. Prepare data with appropriate preprocessing and loading mechanisms.
2. Design a model architecture suited to the problem.
3. Choose appropriate loss functions and optimizers.
4. Train the model while monitoring validation performance.
5. Experiment with different hyperparameters and model adjustments.
6. Select the best model based on validation metrics.
7. Evaluate final performance on test data.
8. If needed, return to step 2 or 3 with new insights.

<p align="center">
<img src="images/visualize.png" alt="Workflow" width="600" height=auto>
</p>
<p align="center">figure:  Training a Neural Network</p>

This methodical approach, while sometimes requiring patience and experimentation, allows us to develop models that can
recognize patterns, make predictions, and solve complex problems across many domains. By understanding each component of
the training process, you gain the ability to diagnose issues, make informed adjustments, and develop increasingly
powerful models for your specific tasks.

<br>
<br>

#### Model Performance

The MNIST dataset is very clean and is one of the few datasets where MLPs and Convolutional Neural Networks perform at a
similar level of accuracy. However, all of the top-scoring architectures for MNIST are CNNs (although their performance
difference compared to MLPs is small). In most cases, CNNs are vastly superior to MLPs, both in terms of accuracy and in
terms of network size when dealing with images.

As we will see, the main reason for the superiority of CNNs is that MLPs have to flatten the input image, and therefore
initially ignore most of the spatial information, which is very important in an image. Also, among other things, they
are not invariant for translation. This means that they need to learn to recognize the same image all over again if we
translate even slightly the objects in it.

CNNs instead don't need to flatten the image and can therefore immediately exploit the spatial structure. As we will
see, through the use of convolution and pooling they also have approximate translation invariance, making them much
better choices for image tasks.

<br>
<br>

#### Model Comparison

<br>
<br>

##### MLPs

1. Structure:

    - Consist of fully connected layers
    - Each neuron connected to every neuron in the previous and next layer

2. Input handling:

    - Typically work with 1D input (flattened data)
    - Lose spatial information when dealing with 2D or 3D data

3. Parameter efficiency:

    - Large number of parameters, especially for high-dimensional input
    - Prone to overfitting on image data

4. Feature extraction:

    - Learn global patterns
    - No built-in understanding of spatial hierarchies

5. Invariance:

    - No built-in translation invariance

6. Applications:
    - Suitable for tabular data, simple pattern recognition
    - Less effective for complex spatial data like images

<br>

##### CNNs

1. Structure:

    - Consist of convolutional layers, pooling layers, and fully connected layers
    - Local connectivity in convolutional layers

2. Input handling:

    - Designed to work with grid-like topologies (e.g., 2D images, 3D videos)
    - Preserve spatial information

3. Parameter efficiency:

    - Fewer parameters due to parameter sharing in convolutional layers
    - Better at generalizing for image-like data

4. Feature extraction:

    - Learn local patterns and build up to more complex features
    - Hierarchical feature learning

5. Invariance:

    - Built-in translation invariance due to convolutional operations

6. Applications:
    - Excellent for image-related tasks (classification, segmentation, detection)
    - Effective for any data with spatial or temporal structure

<br>

The key differences of the MLPs vs CNNs are:

1. Connectivity: MLPs use full connectivity, while CNNs use local connectivity.
2. Spatial awareness: CNNs preserve and utilize spatial information, MLPs do not.
3. Parameter efficiency: CNNs are more parameter-efficient for image-like data.
4. Feature learning: CNNs learn hierarchical features, MLPs learn global patterns.
5. Invariance: CNNs have built-in translation invariance, MLPs do not.

The use cases of the MLPs vs CNNs are:

- Use MLPs for:

    - Tabular data
    - Simple pattern recognition tasks
    - When input features don't have spatial or temporal relationships

- Use CNNs for:
    - Image processing tasks
    - Data with spatial or temporal structure
    - When you need to preserve and utilize spatial information

<br>
<br>

#### Convolutional Neural Networks (CNNs)

Convolutional Neural Networks are characterized by locally-connected layers, i.e., layers where neurons are connected to
only a limited numbers of input pixels (instead of all the pixels like in fully-connected layers). Moreover, these
neurons share their weights, which drastically reduces the number of parameters in the network with respect to MLPs. The
idea behind this weight-sharing is that the network should be able to recognize the same pattern anywhere in the image.

CNNs can preserve spatial information, and the key to this capability is called the _Convolution operation_: it makes
the network capable of extracting spatial and color patterns that characterize different objects. CNNs use filters (also
known as _kernels_) to _extract_ the features of an object (for example, edges). By using multiple different filters the
network can learn to recognize complex shapes and objects.

Image filters are a traditional concept in computer vision. They are small matrices that can be used to transform the
input image in specific ways, for example, highlighting edges of objects in the image. An edge of an object is a place
in an image where the intensity changes significantly.

To detect these changes in intensity within an image, you can create specific image filters that look at groups of
pixels and react to alternating patterns of dark/light pixels. These filters produce an output that shows edges of
objects and differing textures. We will see that CNNs can learn the most useful filters needed to, for example, classify
an image. But before doing that, let's look at some specific filters that we can create manually to understand how they
work.

##### Kernel Convolution

Kernel convolution is the fundamental mathematical operation that allows Convolutional Neural Networks to detect
features in images. Think of it as a spotlight that systematically moves across an image, focusing on small regions at a
time to detect patterns like edges, textures, and shapes. This operation is what gives CNNs their remarkable ability to
understand visual content.

At its core, kernel convolution involves sliding a small matrix of weights (the kernel or filter) over an input image
and performing element-wise multiplication followed by summation at each position. This creates a new image called a
feature map that highlights specific patterns detected by that particular kernel. It’s a fundamental operation in
Convolutional Neural Networks (CNNs) that enables the network to detect features in images. It works as:

1. Kernel (Filter): A small matrix of weights, typically 3x3 or 5x5 as below:

    ```shell
    	[ 0  -1   0]
    	[-1   4  -1]
    	[ 0  -1   0]
    ```

2. Sliding Window: The kernel slides over the input image.

3. Element-wise Multiplication: At each position, the kernel is multiplied element-wise with the overlapping image
   patch.

4. Summation: The products are summed to produce a single output value.

The convolution operation proceeds as follows:

1. Align the kernel with the image patch
2. Multiply each kernel value with its corresponding image pixel
3. Sum all the resulting products

Say, an mage Patch (3x3) is:

```shell
	[140 120 120]
	[225 220 205]
	[255 250 230]
```

The value in the output feature map for this position will be:

```shell
(0*140) + (-1*120) + (0*120) + (-1*225) + (4*220) + (-1*205) + (0*255) + (-1*250) + (0*230) = 60
```

This output value (60) becomes the value in the corresponding position of the output feature map. The kernel then shifts
by a predefined stride (typically 1 pixel) and repeats this process for the next position, eventually covering the
entire image.

###### Why Kernel Convolution Matters

Kernel convolution offers several critical advantages for image processing:

1. **Parameter efficiency**: By sharing the same kernel weights across the entire image, CNNs dramatically reduce the
   number of parameters compared to fully-connected networks.
2. **Spatial hierarchy**: As convolutions are stacked in deeper layers, the network builds a hierarchical
   representation, capturing increasingly complex features from edges to textures to object parts.
3. **Translation invariance**: The same kernel can detect a pattern regardless of where it appears in the image, making
   CNNs naturally robust to object position variations.
4. **Preserving spatial relationships**: Unlike MLPs that flatten images, convolution preserves the spatial arrangement
   of pixels, allowing the network to understand the 2D structure of images.

##### Common Kernel Types and Their Functions

Kernels (or filters) in CNNs act as feature detectors that highlight specific patterns in images. Different kernels are
designed to respond to different visual elements. Understanding these kernels helps us grasp how CNNs "see" images.

###### Edge Detection Kernels

Edge detection kernels identify boundaries between different regions in an image—places where pixel intensity changes
significantly.

The Sobel horizontal edge detector:

```shell
[-1 -2 -1]
[ 0  0  0]
[ 1  2  1]
```

This kernel responds strongly to horizontal edges. The top row has negative weights and the bottom row has positive
weights, creating a strong response when there's a transition from dark (above) to light (below). The middle row of
zeros acts as a "neutral zone" between the detecting regions. The central coefficients (-2, 2) have stronger weights to
emphasize the center of the detection.

The Sobel vertical edge detector:

```shell
[-1  0  1]
[-2  0  2]
[-1  0  1]
```

This kernel detects vertical edges using the same principle, but rotated 90 degrees. The left column has negative
weights and the right column has positive weights, creating a strong response at vertical transitions from dark (left)
to light (right).

###### Other Important Kernel Types

Beyond edge detection, CNNs use various other kernels:

**Sharpening kernel:**

```shell
[ 0 -1  0]
[-1  5 -1]
[ 0 -1  0]
```

This kernel enhances details by amplifying the difference between a pixel and its neighbors.

**Gaussian blur kernel:**

```shell
1/16 * [1 2 1]
       [2 4 2]
       [1 2 1]
```

This reduces noise and detail by averaging neighboring pixels, giving more weight to the central pixels.

**Emboss kernel:**

```shell
[-2 -1  0]
[-1  1  1]
[ 0  1  2]
```

This creates a 3D effect by highlighting edges with shadow and light.

##### Learning Kernels in CNNs

The true power of CNNs comes from their ability to learn the most effective kernels automatically through
backpropagation. While we can manually design kernels like those above, CNNs discover optimal filters for their specific
tasks:

1. Early CNN layers tend to learn kernels similar to edge detectors
2. Middle layers combine these edges to detect shapes and textures
3. Deeper layers learn highly specialized filters that respond to complex objects

This hierarchical learning allows CNNs to build increasingly sophisticated representations from simple building blocks.

##### Edge Handling

When applying convolution, we encounter a practical problem: what happens when the kernel extends beyond the image
boundaries? This is the edge handling problem, and how we solve it significantly impacts our feature maps. The Common
Edge Handling Techniques are:

###### Padding

The most common approach is padding, where we add extra pixels around the image border before applying convolution. With
zero-padding, we add zeros (black pixels) around the edges:

```shell
000000000
0XXXXXXX0
0XXXXXXX0
0XXXXXXX0
000000000
```

Zero-padding offers two main benefits:

- It preserves the original image dimensions in the output feature map
- It gives equal importance to pixels throughout the image, including those at the edges

Without padding, the output feature map would shrink with each convolution layer, eventually losing significant
information about the image boundaries.

###### Cropping

Another approach is to simply avoid placing the kernel where it would extend beyond the image boundary. This results in
a smaller output feature map:

- An n×n image convolved with a k×k kernel (without padding) will produce an output of size (n-k+1)×(n-k+1)
- This approach discards edge information but ensures that all convolution operations use only valid image data

###### Extension

Sometimes we extend the border pixels outward rather than adding zeros:

- Corner pixels extend diagonally in 90° wedges
- Edge pixels extend in straight lines
- This maintains the original image size while avoiding the artificial boundary created by zero-padding

###### Choosing the Right Edge Handling Method

The choice of edge handling technique depends on several factors:

- **Zero-padding** is most common in modern architectures, especially when preserving dimensional information is
  important
- **Cropping** might be preferred when edge artifacts are a concern
- **Extension** can be useful when edge information is particularly important for the task

Understanding both kernel convolution and edge handling provides the foundation for grasping how CNNs process and learn
from images. These operations allow CNNs to systematically extract features while maintaining the spatial relationships
that make visual data meaningful.

##### Pooling

Pooling is a dimension reduction technique that compresses feature maps while retaining their most important
information. Think of pooling as creating a lower-resolution summary of the detected features, similar to how you might
squint at an image to see its essential structure without fine details.

###### How Pooling Works

The pooling operation involves:

1. Defining a window size (typically 2×2)
2. Moving this window across the feature map in strides (usually 2, meaning non-overlapping windows)
3. Computing a summary statistic for each window
4. Creating a new, smaller feature map from these statistics

###### Types of Pooling

###### Max Pooling

Max pooling keeps the maximum value from each window. For example, with a 2×2 window:

```shell
| 3  7 |    →    | 9 |
| 4  9 |
```

Max pooling is the most common type because it preserves the strongest feature activations—essentially keeping the "most
important" signals. It's particularly effective for features like edges where the maximum response indicates the
presence of that feature.

###### Average Pooling

Average pooling computes the mean of all values in the window:

```shell
| 3  7 |    →    | 5.75 |
| 4  9 |
```

Average pooling creates a smoother representation that considers all activations in the region. It's sometimes preferred
for:

- Background or texture information where the overall pattern matters more than peak values
- Later layers where feature maps represent higher-level concepts

###### Global Pooling

Global pooling applies the pooling operation across the entire feature map, reducing each map to a single value:

- Global Max Pooling: Keeps the maximum value from the entire feature map
- Global Average Pooling: Computes the average of all values in the feature map

Global pooling is often used in the final layers of a CNN to create a fixed-length feature vector regardless of input
size.

###### Why Pooling Matters

Pooling provides several critical benefits to CNN architectures:

###### Spatial Dimension Reduction

By reducing feature map dimensions (typically by 75% with 2×2 pooling), pooling:

- Decreases computational requirements
- Reduces memory consumption
- Allows deeper networks with the same computational budget

###### Translation Invariance

Pooling creates a form of positional tolerance—features are detected regardless of their exact location. For example,
max pooling will detect an edge whether it appears in the top-left or bottom-right of the pooling window.

###### Preventing Overfitting

By reducing the spatial resolution, pooling forces the network to focus on larger patterns rather than memorizing
pixel-exact details, which helps prevent overfitting to the training data.

Understanding how different kernel types extract various features and how pooling operations condense this information
is essential for mastering the inner workings of CNNs. These operations work together to transform raw pixel data into
increasingly abstract and meaningful representations that enable powerful image understanding.

### CNN Architecture

A typical CNN block consists of:

1. Convolutional Layer
2. Activation Function (e.g., ReLU)
3. Pooling Layer

#### Concept Abstraction

- Stacking multiple CNN blocks allows the network to learn increasingly complex features
- Early layers detect simple features (edges, textures)
- Deeper layers combine these to detect more complex patterns (shapes, objects)

#### Translation Invariance

- Multiple CNN blocks enable the network to recognize objects regardless of their position in the image
- This is crucial for robust object recognition in various scenarios

#### Image Processing Usage

- CNNs excel at tasks like image classification, object detection, and segmentation
- The combination of convolution and pooling allows for efficient feature learning and dimensionality reduction
- Learned features are often more effective than hand-crafted features

<br>

#### Training Process

1. Forward pass: Apply convolutions and pooling to input image
2. Compare output to ground truth (using a loss function)
3. Backward pass: Compute gradients and update kernel weights
4. Repeat process with many images to optimize kernel weights

By leveraging these concepts, CNNs have revolutionized computer vision tasks, achieving state-of-the-art performance in
numerous applications.

##### Pooling

Pooling is a mechanism often used in CNNs (and in neural networks in general). Pooling compresses information from a
layer by summarizing areas of the feature maps produced in that layer. It works by sliding a window over each feature
map, just like convolution, but instead of applying a kernel we compute a summary statistic (for example the maximum or
the mean). If we take the maximum within each window, then we call this Max Pooling.

<p align="center">
<img src="images/pooling.png" alt="Pooling" width="600" height=auto>
</p>
<p align="center">figure: Pooling operation in a neural network, summarizing feature maps</p>

<br>
<br>

A block consisting of a convolutional layer followed by a max pooling layer (and an activation function) is the typical
building block of a CNN. By combining multiple such blocks, the network learns to extract more and more complex
information from the image. Moreover, combining multiple blocks allows the network to achieve translation invariance,
meaning it will be able to recognize the presence of an object wherever that object is translated within the image.

The _Effective Receptive Field (ERF)_ in Convolutional Neural Networks refers to the area of the input image that
influences a particular neuron in a deeper layer of the network. While the theoretical receptive field might be large,
the effective area that significantly impacts the neuron's output is often smaller and has a Gaussian-like distribution
of influence. Key points:

1. As we go deeper into the network, each neuron indirectly sees a larger portion of the input image.
2. Not all pixels in the theoretical receptive field contribute equally to the neuron's output.
3. Central pixels typically have more influence than those at the edges of the receptive field.
4. The shape of the ERF is often Gaussian-like, with influence decreasing from the center outwards.
5. ERFs evolve during training, potentially becoming more focused or spread out.

<br>
<br>

#### Receptive Fields

The region in the input space that influences a particular CNN feature called the _effective receptive field (ERF)_. It
is a measure of how much a given pixel in the input image influences the output of a given feature map in a CNN. The
differences of the Theoretical vs. Effective Receptive Field are:

- Theoretical: The entire input area that could potentially influence the feature
- Effective: The area that actually has significant influence on the feature

#### Characteristics of Effective Receptive Fields

##### Shape and Distribution

1. Gaussian-like distribution of influence
2. Center pixels have more impact than edge pixels

##### Size

1. Grows larger in deeper layers of the network
2. Affected by kernel size, stride, and network depth

##### Dynamic Nature

1. ERFs evolve during network training
2. Can become more focused or spread out based on the task

<br>
<br>

Methods for calculating the ERF are:

1. Gradient-based approaches
2. Deconvolution techniques

Visualization techniques of the ERF are:

1. Heat maps showing pixel influence
2. Overlays on input images to demonstrate ERF size and shape

#### ERFs Importance in CNN Architecture

##### Feature hierarchies

1. Shallow layers: small ERFs, local features
2. Deep layers: large ERFs, global features

##### Network design considerations

1. Balancing ERF size with computational efficiency
2. Ensuring appropriate ERF growth through the network

##### Object detection and localization

1. ERFs affect the network's ability to capture context
2. Influence on the scale of objects that can be detected

##### Image classification

1. Role in capturing both local and global features
2. Importance for handling different scales of input

<br>
<br>

##### Dilated/Atrous convolutions

1. Technique to increase ERF without increasing parameters
2. Applications in semantic segmentation

##### Attention mechanisms

1. Dynamic adjustment of ERFs
2. Allowing the network to focus on relevant parts of the input

By understanding ERFs, we gain insights into how CNNs process information and how to design more effective architectures
for various computer vision tasks. The concept of receptive field is that a pixel in the feature map of a deep layer is
computed using information that originates from a large area of the input image, although it is mediated by other
layers:

<p align="center">
<img src="images/effective.png" alt="Receptive Field" width="600" height=auto>
</p>
<p align="center">figure: Pooling operation in a neural network, summarizing feature maps</p>

<br>
<br>

In practice things are a bit more complicated. When we compute the effective receptive field, instead of considering
just whether the information contained in a given pixel is used or not by a pixel in a deeper layer, we can consider how
many times that pixel is used. In other words, how many times that pixel was part of a convolution that ended up in a
result used by the pixel in the deeper layer. Of course, pixels on the border of the input image are used during fewer
convolutions than pixels in the center of the image. We can take this even further and ask how much a given pixel in the
input image influences the pixel in a feature map deeper in the network. This means, if we change the value of the input
pixel slightly, how much does the pixel in the deep layer change. If we take this into account, we end up with receptive
fields that are more Gaussian-like, instead of flat as we have simplified them in the video, and they also evolve as we
train the network. Visualization of an effective receptive field showing varying pixel influence in a neural network
feature map.

<br>
<br>

<p align="center">
<img src="images/architecture.png" alt="Customer" width="600" height=auto>
</p>
<p align="center">figure:  CNN Architecture Blueprint</p>

<br>
<br>

<p align="center">
<img src="images/output.png" alt="Customer" width="600" height=auto>
</p>
<p align="center">figure: Pooling operation in a neural network, summarizing feature maps</p>

#### Glossary

_Convolutional Neural Network (CNN)_: A class of Neural Networks featuring local connectivity, weight sharing, and
pooling operations.

_MNIST_: A dataset of handwritten digits of historical importance, commonly used nowadays for tutorials and learning.

_Dataloader_: Allows sequential or random iterations over a dataset or over a subset of a dataset.

_Local connectivity_: In the fully-connected layers present in a Multi-Layer Perceptron the neurons in a layer are
connected to all neurons in the previous layer. Instead, in CNNs, a neuron is connected only to a small portion of
contiguous neurons in the previous layer (or pixels in the input image).

_Filters / Kernels_: Small matrices of numbers, usually normalized to 1, that are applied to the input image during the
convolution operation.

_Convolution_: The operation of sliding a kernel on an image or a feature map to produce a modified output.

_Feature map_: The result of applying a filter on an image or another feature map. One kernel/filter generates one
feature map.

_Pooling_: The operation of sliding a window over the input image or a feature map and applying a function to the
numbers present in that window (for example, taking the maximum).

_Max Pooling_: The operation of sliding a window over the input image or a feature map and applying a maximum function
to the numbers present in that window.

_Sobel filters_: Specific types of filters that can isolate vertical and horizontal edges.

_Effective receptive field (EFR)_: Generally, this is the region in the input image that contributes to the values of a
pixel in a feature map deep in the network. More precisely, the effective receptive field is a matrix superimposed on
the input image, where each element has a value proportional to the importance that a pixel in the input image has in
determining the value of the pixel in the feature map deep in the network.

_Flattening_: The operation of taking an image or a feature map and changing it into a vector (or 1d array).
